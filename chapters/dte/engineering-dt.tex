In an attempt to answer \ref{rq:1}, 
this chapter presents our proposal for the engineering of \acp{DT}
focusing on the challenges of integrating heterogeneous sources of \ac{PA} data
and exposing \ac{DT} functionalities as services for digital applications.
%
Additionally, the chapter addresses the challenge of managing the \emph{lifecycle} of a \ac{DT}, 
which is crucial to ensure that the \ac{DT} can accurately reflect the state of the \ac{PA}, 
or signal when it may not be able to do so, in order for applications to make informed decisions on how to 
interpret the data provided by the \ac{DT}.
%
Finally, the modelling of \emph{augmented} functionalities of a \ac{DT} is discussed, with a modular approach
in order to decouple them from the primary \ac{DT} responsibility of \emph{shadowing} the \ac{PA} state. 
%
The resulting contribution is the proposal of an architectural framework and patterns for the engineering of \acp{DT}, 
which has been implemented in a supporting open-source framework: the \ac{WLDT} Java Library\footnote{\url{https://wldt.github.io}}.

%=======================================================
\section{An Abstract Architecture for \aclp{DT}}
\label{sec:dte:engineering-dt:abstract-architecture}
%=======================================================
Starting from the principles outlined by the 5D model~\cite{dt-driven-prognostics-tao-2018},
and incorporating insights from both recent academic research~\cite{web-of-dt-ricci-2022,Bellavista_Bicocchi_Fogli_Giannelli_Mamei_Picone_2023} and relevant standards from industry bodies~\cite{etsi-dt-comm-requirements-2024}.
This section presents a proposal aiming to generalize an abstract architecture that captures the core software requirements for \ac{DT} development.

%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/mapping-Tao-WLDT.pdf}
    \caption{A side-by-side mapping between 5D DT modelling and the abstract \ac{DT} architecture.}
    \label{fig:tao_mapping_dt_modelling}
\end{figure}
%%%

In the 5D model (Figure \ref{fig:tao_mapping_dt_modelling}, left), 
the first dimension, Physical Entities (PE), represents the actual physical object or system being modeled as a \ac{DT}.
The second dimension, Virtual Entity (VE), is the digital representation of the physical entity, replicating its characteristics and behaviors.
The third dimension, Connection (C), ensures linkage between the physical and virtual entities, including communication technologies, data transmission protocols, and synchronization mechanisms for real-time interaction and data exchange.
The fourth dimension, Data (D), covers data management aspects, ensuring accurate and timely data for simulation, prediction, and optimization.
Finally, the fifth dimension, Service (S), involves various services enabled by the \ac{DT}, such as monitoring, simulation, prediction, control, and optimization, leveraging data and insights from the virtual entity to enhance the performance and efficiency of the physical system.

Moving this towards an abstract architectural perspective, a \ac{DT} can be described as the combination of three main high-level components as schematically illustrated in Figure \ref{fig:tao_mapping_dt_modelling} (right): 

\begin{itemize}
    \item \textit{\ac{PI}:} tasked with both the digitalization process and the ongoing synchronization of the \ac{DT} and \ac{PA} throughout their lifecycle based on its characteristic cyber-physical nature and the supported protocols and data formats (e.g., HTTP and JSON, MQTT and binary);
    \item \textit{\ac{DI}:} complementing the \ac{PI}, it manages the routing of \ac{DT}'s internal variations and events directed towards external digital entities and consumers ensuring the \ac{DT}'s interaction, interoperability and observability;
    \item  \textit{\ac{M}:} Defines the \ac{DT}'s behavior through the digitalization process together with augmented functionalities.
    The \textit{shadowing} process is responsible for handling events from both the \ac{PI} and \ac{DI} to model and replicate the state of the asset,
    while \textit{augmentation}~\cite{dt-IoT-context-Minerva-2020} functions extends functionalities of the \ac{PA} through additional features and capabilities supported by the software nature of the twin (e.g., simulation, machine learning inference models).
\end{itemize}


Table~\ref{tab:evaluation} discusses how the proposed architecture supports the core properties of \acp{DT}~\cite{dt-IoT-context-Minerva-2020} compared to both existing research from the scientific literature and features offered by major production-ready platforms, including Microsoft Azure, Eclipse Ditto, Amazon AWS, and Bosch IoT Suite.

These platforms mainly function as data repositories, storing data from physical assets, with pre-processing handled externally.
Data acquisition pipelines, supported protocols and data formats are often defined at the platform level, not on a per-\ac{DT} basis, leading to limitations in the shadowing process and client interactions.

In contrast, the proposed approach is \emph{event-driven}, where both the \ac{PI} and \ac{DI} are responsible for capturing events from their respective domains (physical and digital) and forwarding them to the \ac{M} for processing. This allows \acp{DT} to operate as independent software entities, communicating with the external world through finely configurable interfaces.

Following the proposed architecture, a \ac{DT} can be formally represented as:

\begin{equation} 
\label{eq:dt_basic_model}
        DT = \langle PI, M, DI \rangle\\
\end{equation}

Where the \ac{PI} captures data from the \acf{PA}, represented as a stream of time ordered \emph{physical} events $e_{PA}$, and controls physical actuation on the object through time ordered actions $a_{PA}$, 
%
the model \ac{M} encapsulates the \ac{PA} behavior, processes physical events $e_{PA}$, and generates time ordered \emph{digital} events $e_{DT}$,
%
and, finally, the \ac{DI} consumes digital events $e_{DT}$, exposes them to external consumers, and supports the invocation of \emph{digital} actions $a_{DT}$ to modify the behavior of the \ac{PA} through the \ac{DT}.

To better align with multimodel perspectives of \acp{DT}, the model \ac{M} can be actually interpreted as a set of models:

\begin{equation}\label{eq:multi_model}
        M = \{ m_{1}, m_{2}, \dots, m_{n} \} \\
\end{equation}


\begin{table}
\renewcommand{\arraystretch}{1.2}
\centering
\small
\begin{tabularx}{\textwidth}{>{\arraybackslash}m{2.5cm} >{\arraybackslash}X}
\hline
\textbf{Property} & \textbf{Description} \\ \hline

\multirow{2}{*}{\parbox[t]{2.8cm}{\raggedright\arraybackslash\hspace{0pt}\textbf{Representativeness \& Contextualization}}}
& \textbf{Traditional:} DTs are limited to store properties and relationships received from external entities, and do not implement a model or a behavior themselves. DTs fail to encapsulate the PA, and the responsibility of the contextualization is entirely externalized. \\
& \textbf{Event-Driven:} DTs are active entities aware of the characteristics of the PAs through dedicated adapters able to support a fine-grained representation and contextualization thanks to internal models and the event-driven approach. \\ \hline

\multirow{2}{*}{\textbf{Shadowing}} 
& \textbf{Traditional:} The shadowing is not correctly encapsulated within a DT and is dispersed across different components. The constraint to rely on a fixed set of communication protocols and platform-specific data formats creates a strong vendor lock-in, limiting re-usability. \\
& \textbf{Event-Driven:} DT directly encapsulates the functionalities and responsibilities to communicate with the PA through the adoption of any required protocols and data formats and decouples the responsibilities of interacting with the PA through re-usable and interoperable modules. \\ \hline

\multirow{2}{*}{\textbf{Replication}} 
& \textbf{Traditional:} Adoption of centralized and monolithic solutions (e.g., Cloud-only) represents a critical limitation to cross-domain interactions, scalability, and fault tolerance since DTs are designed as passive singleton instances operating in isolated tenants. \\
& \textbf{Event-Driven:} The event-driven approach natively supports the creation of multiple active, independent, and modular DTs interacting by means of events and operating through multi-layered and cross-domain deployments. \\ \hline

\multirow{2}{*}{\textbf{Composability}} 
& \textbf{Traditional:} The composition logic and its management (regardless of whether it is linking or an effective interconnection) are fully externalized, and the DT is passively subject to them. \\
& \textbf{Event-Driven:} Each DT is active and directly handles the composition process through behavior defined by its model, supported by dynamic adapters and a native, event-based notion of observability. \\ \hline

\multirow{2}{*}{\textbf{Augmentation}} 
& \textbf{Traditional:} DT augmentation is a concept entirely missing from state-of-the-art platforms, and it is outsourced to external computational services. \\
& \textbf{Event-Driven:} Augmentation is implemented through a flexible plug-in mechanism allowing the injection of novel functionalities and relying on existing modules to effectively augment the DT. \\ \hline
\end{tabularx}

\caption{Comparison of DT capabilities: limitations of existing (traditional) approaches versus the benefits of the proposed event-driven solution.}
\label{tab:evaluation}
\end{table}

In the proposed approach, the core functionality of the \ac{DT} is the shadowing process, which handles the transformation of data received from the \ac{PI}, feeding the models that characterize the \ac{DT}.
%
The result of the shadowing process is to compute the state of the \ac{DT} $S_{DT}$, which---following the \ac{WoDT} metamodel~\cite{web-of-dt-ricci-2022}---can be represented through sets of: 
\begin{itemize}
\item \textbf{Properties} ($P$): labeled data that change with the evolution of the \ac{PA} state;
\item \textbf{Relationships} ($R$): capturing the existing and dynamic connections among \acp{PA} within the system, mirroring them between the corresponding \acp{DT}; 
\item \textbf{Events} ($E$): non-persistent signals captured by the information gathered from the associated \ac{PA};
\item \textbf{Actions} ($A$): the possible operations that the \ac{DT} allows to be invoked on behalf of the \ac{PA}, to either send feedback to the physical entity or exploit a service exposed by the \ac{DT}.
\end{itemize}

At any given time $t$, the state of the \ac{DT} can hence be represented as: 

\begin{equation}
    S_{DT} = \langle P, R, E, A, t \rangle \\
\end{equation}

Computed states can be stored to maintain a history $H$ of the \ac{DT}, which can be modeled as a time ordered sequence of states:

\begin{equation}\label{eq:dt_history}
        H = \langle S_{DT_1}, S_{DT_2}, ... , S_{DT_n}\rangle\\
\end{equation}

Additionally, to fulfil the \emph{shadowing} process, 
state updates can then be communicated to applications through the \ac{DI}, fulfilling the objective of the \ac{DT} in providing an up-to-date representation of the \ac{PA}.
%
We can represent the shadowing as a function $Shad$ that given an event from the \ac{PI} $e_{PA}$, the set of models $M$, and the history of previous states $H$, computes the new state of the \ac{DT} and generates an event $e_{DT}$ to be sent through the \ac{DI}:

\begin{equation}
        e_{DT}(S_{DT}) = Shad_{PA \rightarrow DT}(M, e_{PA}, H) \\
\end{equation}

The shadowing is bidirectional:
when actions are invoked, the \ac{DT} receives an action request $a_{DT}$ on its \ac{DI}, validates it through the models $M$ possibly evaluating the history $H$, and then it may propagate the request through its \ac{PI} sending a command $a_{PA}$ to have an effect on the \ac{PA}.

\begin{equation}
    a_{PA} = Shad_{DT \rightarrow PA}(M, a_{DT}, H)
\end{equation}

It is crucial to note that a digital action request does not directly change the state of the \ac{DT}.
Any changes of the state occur only as a result of the propagation of physical changes from the \ac{PA} to the \ac{DT}.
This abstract architectural model addresses the main functional requirements of a \ac{DT}.
It provides a clear separation of concerns between different components focusing on the \emph{shadowing} process as the core responsibility of the \ac{DT} encapsulating the complexity of physical and digital interactions through the \ac{PI} and \ac{DI}. 


Finally, considering these additional specifications, the resulting \ac{DT} model enhances (\ref{eq:dt_basic_model}) to include also the shadowing $Shad$ and the history of states $H$ in the \ac{DT} definition:

\begin{equation}\label{eq:5D_dt_model}
    DT = \langle PI, M, Shad, H, DI \rangle
\end{equation}

In the following sections the abstract architecture will be refined to highlight how it can concretely be implemented to address challenges in \ac{DT} engineering.


%=======================================================
\section{Modular Design of Digital Twin Interfaces}
%=======================================================

As introduced in \Cref{sec:back:dt:interoperability}, interoperability is a key requirement for \ac{DT} development~\cite{Acharya_Khan_Päivärinta_2024,Klar_Arvidsson_Angelakis_2024}.
%
Considering interoperability as the ability to integrate effectively with other system components,
in the \ac{DT} context, such ability can be considered twofold due to their cyber-physical nature.

Figure \ref{fig:dt_application_example} represents a \ac{DT} implemented following the abstract architecture presented in Section \ref{sec:dte:engineering-dt:abstract-architecture} in an exemplary deployment setting.
%
It highlights the role of the \ac{DT} as a bridge between \emph{physical} devices and \emph{digital} applications.

On the one hand, \acp{DT} must effectively integrate with a heterogeneous physical world, interacting with diverse \ac{IoT} devices and systems that operate on different standards, protocols, and architectures.
%
For example machines from different vendors may utilize different communication protocols and interaction patterns.
%
Similarly, in healthcare applications, \acp{DT} must integrate various electronic health records across different data sources, from traditional repositories to sensor networks.
%
This \emph{physical} heterogeneity poses a challenge for the development of \acp{DT}, as, most often, it is unfeasible to modify the existing conditions and the \ac{DT} system must be constructed on top of the legacy ones \emph{adapting} to their requirements.

On the other hand, although \acp{DT} could work in isolation, they are increasingly being envisioned as part of complex software systems.
%
Thus, \acp{DT} needing to integrate with different kinds of services ranging from system automation tools, to data platforms, machine learning pipelines and visualization tools, may expect to face several degrees of \emph{digital} heterogeneity.
%
As for the physical realm, the \ac{DT} might need to \emph{adapt} to the requirements of the digital ecosystem it is part of.
%
Additionally, the modeling of complex \ac{CPS} can be tackled through the idea of \acp{DTE}~\cite{web-of-dt-ricci-2022} which envision multiple connected \acp{DT}, each modeling individual entities of the same domain. 
Such \acp{DTE} can benefit from standardized interactions between \acp{DT} and with higher-level services.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/dt_application_example.pdf}
    \caption{Example of a DT in an application scenario interacting with a physical asset and digital applications through its PI and DI.}
    \label{fig:dt_application_example}
\end{figure}


Tackling such twofold interoperability challenges requires on one side
the adoption of common data models, semantic interoperability frameworks, and compliance with industry standards~\cite{etsi-dt-comm-requirements-2024} such as OPC-UA\footnote{OPC-UA: \url{https://opcfoundation.org/about/opc-technologies/opc-ua/}}, oneM2M\footnote{oneM2M: \url{https://www.onem2m.org/}}, ISO-23247\footnote{ISO-DigitalTwin: \url{https://www.iso.org/standard/81442.html}}, or the W3C Web of Things\footnote{W3C WoT: \url{https://www.w3.org/WoT/}}.
%
On the other side, the \ac{DT} architecture should accommodate physical and digital heterogeneity \emph{by design} to avoid being locked down in those situations where adherence to a single standard can not be enforced.
Without such measures, \acp{DT} risk becoming fragmented, limiting their ability to collaborate and exchange information across different platforms and applications.
%
The proposal of modular \ac{DT} interfaces aims to address these concerns
at the architectural level, to enhances the interoperability and re-usability of \ac{DT} components in cyber-physical applications.

At an appropriate abstraction level, a \ac{PA} may be digitalized by obtaining and sending data through possibly \emph{several} different \texttt{Communication Channels} (\Cref{fig:dt_application_example}), which 
encompass network protocols, data formats, and physical connections.
The \ac{PI} manages interactions with the \ac{PA}, integrating such channels.
%
Similarly, on the digital side, the \ac{DT} supplies data and insights to applications via the \ac{DI}, adapting to different protocols and representations.

Engineering the \ac{PI} and \ac{DI} requires addressing the challenge of effectively integrating such heterogeneous communication channels.
%
To ensure flexibility and adaptability, it is essential that the core of the \ac{DT}, including its models and implemented behaviors, remains decoupled from the heterogeneous physical and digital communication channels.
%
The \ac{DT}'s shadowing process should focus solely on understanding the available physical data, receiving and transmitting telemetry information, and executing action requests, with no concern for the underlying implementation details.

The remainder of this section describes a proposal for the design the \ac{DT}'s \ac{PI} and \ac{DI}, refining and extending the conceptual model originally proposed in \cite{web-of-dt-ricci-2022} with a concrete implementation based on the concept of modular \emph{adapters}.

%..........................................................
\subsection{Physical Interoperability}
\label{ssec:dte:dt-engineering:physical_interoperability}
%..........................................................

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/dt-interoperability/dt_interoperability_physical.pdf}
    \caption{PI design with modular physical adapters each producing the corresponding \acl{PAD} that is processed by the \ac{DT} Model.}
    \label{fig:physical_interoperability}
\end{figure}


One of the main challenges in facilitating effective communication through the \ac{PI} is the wide variety of communication protocols and data formats used by \acp{PA}.
%
While it can be the case that one \ac{PA} is directly sending all the data concerning its digitalization through only one channel,
it is far more common to build a \ac{DT} aggregating different sources of information~\cite{qi2018dt-and-bigdata}.
%
\ac{IoT} protocols often differ considerably based on the manufacturer, device type, or application domain, making it difficult for the \ac{DT} software to integrate diverse assets.
%
Arguably, the modularity of the \ac{PI}, which encapsulates these responsibilities, is a crucial factor in the design and implementation of interoperable \acp{DT}.
%
Accordingly, the proposal is to design the \ac{PI} as a composition of multiple \emph{\acp{PhA}}: specialized modules capable of interacting with the \ac{PA} using diverse protocols and data formats.
%
As shown in Figure \ref{fig:physical_interoperability}, each \ac{PhA} is responsible for mediating the bidirectional interaction through a single communication channel, simplifying the design and reusability of the component, and making it configurable to adapt to different application contexts.
%
The responsibility of the \ac{PI} becomes then to manage the different \acp{PhA} and make sure the \ac{DT} model can accurately interpret, process, and leverage the data generated by the physical world to create the digital replica and implement its behaviors.
%
Note that although terminology of \ac{PhA} is borrowed from \cite{web-of-dt-ricci-2022}, where it is originally introduced, the \emph{Physical Asset Adapter} proposed in \cite{web-of-dt-ricci-2022} is in fact a conceptual monolithic component, whereas in this concrete proposal a \ac{PhA} is considered as a single-responsibility module of a potentially more complex \ac{PI}.

\paragraph{Generating \aclp{PAD}}
To facilitate managing different \acp{PhA}, the concept \emph{\ac{PAD}} is introduced: a representation of the capabilities provided by a communication channel in terms of \emph{properties}, \emph{actions}, \emph{events}, and \emph{relationships} that characterize the associated \ac{PA}.
%
Each \ac{PhA}, since it encapsulates the characteristics of a channel, can generate the corresponding \ac{PAD}, effectively decoupling the asset's functionality from the specific communication protocols used.
%
The implementation of the \ac{PAD} generation can be challenging due to the varying capabilities of different communication protocols.
For instance, protocols like \ac{CoAP}~\cite{RFC7252} often come with built-in description and discovery functionalities, which can simplify the process of creating a \ac{PAD} by providing standardized representations of the physical asset's capabilities and behaviors.
On the other hand, protocols like \ac{MQTT}~\cite{MQTTv5} may require developers to add additional information manually to define how information is structured and exchanged, as they do not natively include asset metadata.
In the case of custom or proprietary protocols, the challenge becomes even more pronounced, as these protocols are tailored to specific systems and may lack any standardization or descriptive capabilities.

In all the aforementioned scenarios, the mechanism of \ac{PAD} generation offers a way to encode knowledge about the \ac{PA} bridging the gap by interpreting and extracting relevant information from the protocol used in the associated communication channels.
%
Confining this complexity within the \ac{PI} allows the \ac{DT} model to be agnostic with regard to the complexity of the underlying physical world.

The \ac{PAD} allows the \ac{PI} to discover, extract, and manage asset information and present it to the model that can choose which ones are relevant for the implementation of the \ac{DT} behavior.
%
For example, to create the \ac{DT} of a temperature sensor streaming binary data on MQTT, the \ac{PI} could be composed of a generic MQTT adapter, configured to correctly parse the payload as a decimal number, and generate a \ac{PAD} which advertise the available temperature property to the \ac{DT} model as a Celsius value. 


\subsection{Digital Interoperability}
\label{sec:digital_interoperability}

%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/dt-interoperability/dt_interoperability_digital.pdf}
    \caption{Digital Interface design with modular adapters and DT description.}
    \label{fig:digital_interoperability}
\end{figure}
%%

As \acp{DT} are meant to bridge between the physical and digital spaces,
interoperability is not only a matter of interfacing with heterogeneous devices, but also other digital applications.
Indeed, despite their initial conception as vertical silos, the concept of \ac{DTE} (\Cref{sec:back:dt:dte}) is emerging to support the the digitalization of complex scenarios through a combination of several \acp{DT}.
%
In this context, \acp{DT} can be used \emph{as-a-service} by other applications implementing the business logic by spanning across different software entities.

To foster interoperability in \acp{DTE}, \acp{DT} must then expose either a standardized general purpose \acf{DI} that can serve different applications or
---following the same design principles adopted to address physical interoperability---
have a modular interface that can satisfy the different needs of different applications
as shown in Figure \ref{fig:digital_interoperability}.

As for \ac{PhA} the terminology is borrowed from \cite{web-of-dt-ricci-2022} and consider to have a \ac{DI} composed of modular \emph{\ac{DiA}}.
The original abstract concept of \ac{DiA} is hence refined to represent a modular component of a more complex \ac{DI}.
Using the concept of \ac{DiA}, the \ac{DI} can expose the \ac{DT} state and services supporting multiple data formats and interaction patterns.
This can be beneficial for integrating it with applications and, especially, legacy systems.
The existence of legacy applications usually implies having little control over the integration requirements, making having more flexible \acp{DT} beneficial to better integrate with the application requirements.

Using several \ac{DiA}, a \ac{DT} could, for instance, support both request-response and publish-subscribe mechanisms to access its current and previous states, support different query languages to access the same data store, expose its current state using different representation formats, etc.
%
This would make the development of the application simpler because adding an application-specific \ac{DiA} won't require intervention in the underlying physical system. 
%
Additionally, the developed application would be more robust and stable since it would depend only on the \ac{DT}, and changes to the physical configuration of the \ac{PA} (e.g., software updates, sensor replacement, network reconfiguration) would have no impact on the application software.
%
Even if the \ac{PA} were to change, (e.g., a software update on a robot changes the telemetry format) the \ac{DI} of the \ac{DT} could stay the same, as the changes would occur within the boundaries of the \ac{PI} and \ac{DT} model.
%
Through this mechanism, \acp{DT} effectively achieve their bridging role, shielding applications from the complexity of physical deployments.

\paragraph{Describing \aclp{DI}}

A further level of interoperability is possible when allowing \acp{DT} to describe their \ac{DI}, advertising capabilities and available communication channels that applications can exploit.

This is especially relevant in contexts where applications are not bound to a specific interface but can instead process machine-readable descriptions of \acp{DT} and choose to interact with specific assets.
%
A \ac{DT} could then use a \emph{\ac{DTD}}, which, similarly to the \ac{PAD}, can represent the features of the \ac{DT} to its observers.
%
The idea of exposing \acp{DTD} is somewhat present in the major platforms supporting the creation of \acp{DTE}, such as 
Microsoft's \azureTwin{}\footnote{\azureTwin{}: \url{https://azure.microsoft.com/en-us/products/digital-twins}} and \ditto{}\footnote{\ditto{} \url{https://eclipse.dev/ditto/}} and is advocated by standardization activities on interoperability in \acp{DTE}~\cite{etsi-dt-comm-requirements-2024}.

The way such descriptions are implemented may differ significantly, but essentially, they should at least allow representing the \ac{DT}  features and APIs to access them.
%
Notably, differently from the \ac{PAD}, the \ac{DTD} is targeted to external consumers, hence, it should preferably adhere to standard formats and representations to be useful in practice in achieving interoperability.
%
To this end, using Semantic Web technologies (see \Cref{sec:back:web:semantic-web-technologies}) could be a possible solution to implement standard machine-readable \acp{DTD}.
\todo{add forward ref to semantic web descriptors sections}

% \begin{figure*}[t]
%     \setlength{\belowcaptionskip}{-13pt}
%     \centering
%     \includegraphics[width=0.93\linewidth]{figures/dt-interoperability/mf_dt_structure.pdf}
%     \caption{The \acl{DT} ecosystem architecture of the microfactory industrial system.}
%     \label{fig:mf_dt_ecosystem}
% \end{figure*}

\subsection{\acl{WoT}: enabling DT Interoperability}
\label{sec:wot-dt-interop}

\ac{WoT}~\cite{wot-arch} standards (\Cref{chap:back:web:WoT}) share the goal of adopting uniform API and data format descriptions to hide low-level complexities and promote interoperability with the proposal of self-descriptive \ac{PhA} and \ac{DiA}.

Despite their similarities, \acp{PAD} and \acp{TD} serve distinct purposes. 
A \ac{TD} provides a structured description of a physical twin's available interactions to external consumers, detailing protocols and interaction possibilities.
%
In contrast, a \ac{PAD} is designed for internal use by the \ac{DT} modules, decoupling the twin's core from the complexities of communication channels.
Its primary role is to facilitate the discovery of available resources and capabilities on the \ac{PA} after establishing a connection with the physical counterpart, providing a structured description of its functionalities and data.
This description is directly interpretable by the \ac{DT} model and independent of the physical characteristics, allowing the reuse of the same model for similar \acp{PA} employing different communication channels.

In contexts in which \ac{WoT} standards are applicable, the devices' \acp{TD} can be automatically mapped to \acp{PAD}, streamlining the \ac{DT} creation process.
Leveraging \ac{WoT} \acp{TD} can significantly reduce the effort required to generate \acp{PAD}, particularly in environments where numerous physical twins already have associated \acp{TD}.
This convergence between \ac{WoT} and \ac{DT} architectures has the potential to accelerate the development and deployment of \ac{DT} solutions by promoting standardization and reusability.
%
Nevertheless, the more general concept of \ac{PAD} can be tailored also to those scenarios in which \ac{WoT} is not applicable. In those cases the responsibility falls back to development (or configuration) of a \ac{PhA} for a specific device to encode the necessary knowledge for the generation of the \ac{PAD}.

\ac{WoT} \acp{TD} can also be used as \acp{DTD}.
The \ac{WoT} architecture actually lists \acp{DT} as one of the possible deployment patterns, with a \ac{DT} mediating the interaction with a \ac{PA} behind a \ac{WoT} interface\footnote{\url{https://www.w3.org/TR/2023/REC-wot-architecture11-20231205/\#digital-twins}}.
%
This is especially useful when devices are not \ac{WoT} compatible or can not be made so due to other constraints, essentially retrofitting the capabilities of the devices with a \ac{WoT} compatible interface.
%
Adhering to \ac{REST} constraints~\cite{fielding2000architectural} and specifically to the HATEOAS and self-descriptive principles, a \ac{TD}-based \ac{DTD} facilitates the discovery of the \ac{DT} model and services.
%
Its machine-readable nature ensures interoperability for applications and consumers.
%
In particular, a \ac{TD}-based \ac{DTD} facilitates the inclusion of \acp{DT} in \ac{WoT} mashups, promoting \acp{DT} as valid \ac{WoT} Things usable by \ac{WoT} Consumers.

Despite its flexibility,
and broad applicability, 
using of \ac{TD} for \acp{DT} presents several limitations in capturing the specific characteristics of \acp{DT} that distinguish them from \emph{Things}.
%
A path to address these limitations could be extending \acp{TD} or supporting additional descriptions specifically for \acp{DT} in \acp{DTE} as explored in \Cref{chap:dte:hwodt}.



%=======================================================
\section{Managing the Digital Twin Lifecycle}
%=======================================================


Recently, there has been an increasing recognition of the importance of the lifecycle of \acp{DT},
particularly in distinguishing the properties that define the relationship between the \ac{DT} and \ac{PA}. 
%
Key concepts such as \emph{reflection} and \emph{entanglement} are critical for accurately representing the \ac{PA}~\cite{dt-IoT-context-Minerva-2020,web-of-dt-ricci-2022}.
%
These properties underscore the necessity for a structured lifecycle for the \ac{DT}, ensuring that its state remains consistently aligned with the \ac{PA} throughout various stages.
%
As highlighted in recent surveys~\cite{ferko2022architecting, 9640612,Hribernik_Cabri_Mandreoli_Mentzas_2021}, while the body of literature on \ac{DT} software architectures is growing, most papers are domain-specific and focus on reference models.
%
However, these models often lack concrete guidance on how to implement the internal components of a \ac{DT}. Many of the existing models envision multiple parallel components or models working together, but fail to address how they communicate and interact to maintain consistency in the \ac{DT} state.
This gap leads to potential inconsistencies between the \ac{DT} and \ac{PA}, as the interaction between models and the management of state changes is not adequately captured~\cite{alam2017access,Malakuti2019fourlayer,Lpez2021}.


% In \cite{Redelinghuys2019}, a six-layer architecture for \acp{DT} is proposed, where the first two layers are dedicated to the \ac{PA} (sensors and controllers), and the third to the fifth layers manage data storage, communication, and cloud integration. However, the architecture lacks a clear process for how these layers communicate with each other or how changes in the \ac{DT} state are captured and updated across the layers. Similarly, in the Generic \ac{DT} Architecture (GDTA)~\cite{app10248903}, a layered approach is used, where the \ac{DT} state is computed at the \emph{information layer} through data processing pipelines. However, this architecture does not incorporate an explicit mechanism for the integration of the different components of the \ac{DT}, leaving gaps in how the evolving state of the \ac{DT} is managed. Other models explicitly reference multiple components that contribute to the definition of the \ac{DT} state. For example, in \cite{alam2017access} and similarly in \cite{Malakuti2019fourlayer}, the authors propose multi-layer \ac{DT} architecture for cyber-physical systems (CPS), where \ac{DT} state changes are driven by multiple functional units. However, in these models, the outputs of each unit are not mediated by a shadowing process, which would ensure consistency and synchronization of the \ac{DT} state and lifecycle management. Our approach addresses these issues by proposing a more structured lifecycle for \acp{DT}, where the communication and interaction between the different components are clearly defined. By introducing a shadowing process that orchestrates the different models, we ensure that the \ac{DT} state remains consistent and accurately reflects the evolving state of the \ac{PA}.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/wldt_lifecycle_simple.pdf}
    \caption{DT Life Cycle with its phases and transitions.}
    \label{fig:dt_life_cycle}
\end{figure}


%START FROM HERE

%-------------------------------------------------------
\subsection{A Digital Twin Synchronization Lifecycle}
%-------------------------------------------------------

The concept of a \ac{DT} lifecycle has recently been introduced and examined, focusing on its core components and the integration of both the software nature of the \ac{DT} and its ability to synchronize with the \ac{PA} over time~\cite{web-of-dt-ricci-2022}.
%
This lifecycle encompasses the various phases that a \ac{DT} undergoes, as shown in Figure~\ref{fig:dt_life_cycle}, from its creation to deactivation.
%
Since a \ac{DT} is fundamentally a software entity, it is vital to track the evolution of such phases considering the different stages of synchronization with its \ac{PA}.
Proper observation and modeling of this lifecycle are essential to ensure that the \ac{DT} representation can be trusted to reflect the physical counterpart's state consistently when the \ac{DT} is \texttt{Synchronized} and can instead be considered to be outdated in the other phases.

The execution lifecycle of a \ac{DT} can be modeled as follows.
Upon creation (from the \texttt{Started} phase),
the \ac{DT} enters the \texttt{Unbound} phase, where all internal modules are initialized and prepared for the binding process with the \ac{PA}.
%
Once the \ac{DT} successfully binds to its \ac{PA},
meaning the \ac{PA} meets the necessary requirements (expected functionalities and properties)
and the \ac{DT} can communicate with it,
the \ac{DT} enters the \texttt{Bound} phase.
%
During this phase, the \ac{DT} is connected to the \ac{PA} and is ready to begin the shadowing process.

The next phase is the \texttt{Synchronized} phase, 
this is different from simply having established a connection with the \ac{PA}, 
but rather indicates that the \ac{DT} is complying with application-specific requirements
and is receiving the necessary and correct amount of information to be fully aligned with its \ac{PA}.
%
During the synchronization phase the \ac{DT} can compute the \ac{DT} state ($S_{DT}$), ensuring that it is able to accurately reflects the status of its physical counterpart.

If any issues arise, such as network failures, and the \ac{DT} synchronization performance falls under the expected requirements, the \ac{DT} enters the \texttt{Out of Sync} phase.
In this state, the \ac{DT}---while still operational---is unable to update its state reliably. 
%
Eventually, the \ac{DT} may return to the \texttt{Synchronized} phase.

Finally, when the \ac{DT} is no longer required or has completed its function, it transitions to the \texttt{Done} phase.
In this phase, the \ac{DT} remains accessible to external consumers and retains its memory, but it is no longer bound to the \ac{PA} or in sync with it. At the end of the lifecycle, the \ac{DT} can be dismissed and moved to the \texttt{Stopped} phase.
Throughout its lifecycle, the \ac{DT} may also return to the \texttt{Unbound} phase if errors are detected during the binding process or if it is rebooted or restarted.


%-------------------------------------------------------
\subsection{Binding the DT with the PA}
%-------------------------------------------------------

The transition from the Unbound to Bound phase in the \ac{DT} lifecycle remains an open research area:
a key challenge is identifying whether all the capabilities of the \ac{PA} relevant to support the \ac{DT}'s behavior are available so that the shadowing process can start.

The modular design of the \ac{PI} presented in the previous section (\Cref{ssec:dte:dt-engineering:physical_interoperability}) can have support this transition.
%
Namely, once a \ac{PhA} successfully connects to the \ac{PA}'s channel and starts receiving data from it, it can send the generated \ac{PAD} to the \ac{DT} model.
The generation of the \ac{PAD} can be used as a synchronization step to signify that the \ac{PhA} is successfully connected to the \ac{PA}.
%
The \ac{DT} model is then responsible for collecting the different \acp{PAD}, assessing whether all the relevant information to start computing the \ac{DT} state is available and consequently moving on to the \texttt{Bound} phase.
%
This mechanism enables managing the \ac{DT} behavior consistently, even with the additional complexity of the modular \ac{PI} design.

Of course the challenge still stands in managing each individual \ac{PhA} connection, but this decoupling through \acp{PAD} allows \ac{DT} developers to establish functional binding requirements that are independent of technical details. 

%--------------------------------------------------------
\subsection{Managing DT and PA Synchronization}
%--------------------------------------------------------


The most significant challenge in current \ac{DT} lifecycle modeling is the broad characterization of the \texttt{Synchronized} phase, which
only generically considers synchronization requirements between the \ac{PA} and the \ac{DT}, without having explicit awareness of the \ac{PA} lifecycle and the potential changes in its operational context.
This general and unstructured approach limits the ability to model the lifecycle in a consistent and uniform manner, failing to properly address the dynamism of the \ac{PA} behavior.

The \texttt{Synchronized} phase of the \ac{DT}'s lifecycle assumes a continuous exchange of information between the \ac{DT} and its associated \ac{PA} usually measured to maintain a frequency within a given threshold, to guarantee that information on the \ac{DT} is \emph{fresh} and can hence be trusted as a valid representation of the \ac{PA} state. 

However, this general assumption may not always hold true due to variations in the \ac{PA}'s internal states.
% The issue arises because \ac{PA} can adjust telemetry frequency and have different value ranges over time, depending on the phases of their lifecycle and their operational context (e.g., from \texttt{Ready} to \texttt{Working}).
% These variations introduce complexity in maintaining the cyber-physical alignment. 
% Without appropriate modeling, these changes might be misinterpreted as failures while they are instead expected variations due to the \ac{PA}'s phase transition.
%
In some cases, \acp{PA} can enter operational states that temporarily inhibit their ability to communicate, even while maintaining an active connection to the \ac{DT}.
These are scenarios in which the absence of messages from the \ac{PA} does not necessarily indicate a failure or misalignment but rather reflects the \ac{PA}'s operational context.
For instance, a \ac{PA} might enter a \texttt{Rebooting} state during which it cannot send telemetry data.
Similarly, resource constraints on the \ac{PA}, such as \textit{CPU overload} or \textit{network bandwidth exhaustion}, may result in disrupted or paused communication that may or may not be tolerable for the \ac{DT} model and application.
%
In other cases, the \ac{PA} may not entirely cease communication but instead alter its update frequency in response to operational changes. For example, an industrial robotic arm might increase its data transmission frequency during a high-precision assembly task to provide real-time feedback, while it could reduce the update rate to conserve energy and network bandwidth when in an idle or maintenance mode.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/dt-lifecycle/dt_lifecycle_pt_sync.pdf}
    \caption{Schematic representation of the \ac{DT} lifecycle with a structured example of the Synchronized phase.}
    \label{fig:dt_lifecycle_pt_sync}
\end{figure}


This adaptive behavior necessitates that the \ac{DT} \emph{dynamically adjust its expectations} and processing strategies based on the \ac{PA}'s operational state, avoiding false-positive anomalies caused by \emph{intentional} communication variability.  
By distinguishing between inhibited communication (e.g., \texttt{Rebooting} or \texttt{Overloaded}) and adjusted communication patterns (e.g., frequency scaling in \texttt{Idle} vs. \texttt{Active} states), the \ac{DT} can better align with the \ac{PA}'s lifecycle.
This alignment ensures the \ac{DT} remains a reliable digital counterpart, accurately reflecting the \ac{PA}'s operational context and providing a robust foundation for decision-making.  

To address these situations, the \ac{DT} must incorporate mechanisms to: 
\begin{itemize}
    \item \emph{Detect and Classify Non-Communication States:} Recognize when the lack of communication from the \ac{PA} is due to a valid operational state (e.g., rebooting) rather than a system failure;
    \item \emph{Model \ac{PA} State-Dependent Communication Behavior:} Explicitly account for \ac{PA} states that imply non-communication, such as \textit{idle}, \textit{rebooting}, or \textit{overloaded}, within the lifecycle framework. 
\end{itemize}
This requires extending the \ac{DT} lifecycle model (\Cref{fig:dt_life_cycle}) to represent such scenarios accurately.


The proposal is to refine the \texttt{Synchronized} phase by introducing sub-phases that correspond to the \ac{PA}'s operational states (\Cref{fig:dt_lifecycle_pt_sync}).
For example, in the industrial domain, a \ac{DT} of a machine may enter the \texttt{Synchronized} phase but should then transition through more specific sub-phases such as \texttt{Ready}, \texttt{Working}, or \texttt{Busy}.
Each of these sub-phases would have clearly defined transition criteria, phases, and associated $S_{\ac{DT}}$, providing a more granular and accurate representation of the \ac{DT}'s behavior.

To formalize this approach, the overall \ac{DT} lifecycle phase at a given time $t$ $LP_{DT}(t)$ can be defined as a composition of the \ac{PA} lifecycle phase $LP_{PA}(t)$ and the \ac{DT} software lifecycle phase $LP_{Soft.}(t)$, as shown in Eq.~\ref{eq:lpdt_definition}.

\begin{equation}
LP_{DT}(t) = \langle  LP_{PA}(t), LP_{Soft.}(t)\rangle
\label{eq:lpdt_definition}
\end{equation}

The \ac{DT} representation of the asset $R_{DT}$ can hence be characterized at any given time instant $t_i$ by its lifecycle phase $LP_{DT}(t_i)$, its current state $S_{DT}(t_i)$, and its history $H(t_a, t_b)$ over a specific time interval $(t_a, t_b)$, as shown in Eq.~\ref{eq:dt-definition}.

\begin{equation}
R_{DT}(t_i) = \langle LP_{DT}(t_i), S_{DT}(t_i), H(t_a, t_b)\rangle \quad \text{and} \quad  t_a, t_b \leq t_i
\label{eq:dt-definition}
\end{equation}

The values of the different components of the \ac{DT} at the time instant $t_i$ can then be mapped as a function of the different possible values of $LP_{Soft.}$, as detailed in Eq.~\ref{eq:lpdt_complete_mapping}.

\begin{equation}
\begin{split}
R_{DT}(t_i) = {} &
\begin{cases}
    LP_{PA} = \emptyset, \ S_{DT} = \emptyset, \ H = \emptyset,\\
    \quad \text{if } LP_{Soft.} \in \{\texttt{Started}, \texttt{Stopped}\}, \\[0.4em]
    LP_{PA} = \emptyset, \ S_{DT} = \emptyset, \ H = H(t_f, t_o),\\
    \quad \text{if } LP_{Soft.} \in \{\texttt{UnBound}, \texttt{Bound}, \texttt{OutOfSync}\}, \\[0.4em]
    LP_{PA} = LP_{PA}(t_i), \ S_{DT} = S_{DT}(t_i), \ H = H(t_f, t_i),\\
    \quad \text{if } LP_{Soft.} = \texttt{Synchronized},\\[0.4em]
    LP_{PA} = \emptyset, \ S_{DT} = \emptyset, \ H = H(t_f, t_d),\\
    \quad \text{if } LP_{Soft.} = \texttt{Done}
\end{cases}
\end{split}
\label{eq:lpdt_complete_mapping}
\end{equation}

When $LP_{Soft.}$ is \texttt{Started}, it represents the initialization phase where the \ac{DT} software has been instantiated. At this stage, $LP_{PA}$ is undefined since no communication with the \ac{PA} has been established. 
Similarly, $S_{DT}$ has not been computed, as the \ac{DT} has not acquired any state information. The history $H$ is also empty, as no synchronization or state updates have occurred yet.

When $LP_{Soft.}$ is \texttt{Unbound}, it indicates that the \ac{DT} is operational but not yet synchronized.
In this phase, $LP_{PA}$ remains undefined because the \ac{PA} lifecycle phase cannot be detected.
$S_{\ac{DT}}$ is also undefined since the \ac{DT} can not compute the state yet.
However, the history $H$ retains information about events recorded from the first connection data received from the \ac{PA} $t_{f}$ to the moment the \ac{DT} transitioned out of the synchronized phase, marked by $t_{o}$.
%
The same happens when the \ac{DT} is \texttt{Bound}, as the state has not been computed yet, 
or if the \ac{DT} is \texttt{Out of Sync}, as the state can no longer be updated.

When $LP_{Soft.}$  is \texttt{Synchronized}, the \ac{DT} is fully synchronized with the \ac{PA}.
In this state, $LP_{PA}$ corresponds to the lifecycle phase of the PA (e.g., in the example of Fig.~\ref{fig:dt_lifecycle_pt_sync}, this could be the \texttt{Working} phase) at the time $t_i$.
At this point, $S_{DT}$ contains the current computed state of the \ac{DT}, reflecting the alignment between the \ac{DT} and \ac{PA}.
The history $H$ encompasses all events and states up to the current time $t_{i}$, documenting the \ac{DT}'s evolution in sync with the PA.
If the \ac{DT} operates correctly, it can remain in the \textit{Synchronized} phase for the duration of its lifecycle, continually updating as the PA's lifecycle progresses.
During this phase, multiple records of lifecycle evolution can be captured as $LP_{PA}$ evolves (e.g., moving from \texttt{Working} to \texttt{Busy}), and multiple computations of $S_{DT}$ may occur within the same $LP_{PA}$.
For instance, while in the \texttt{Working} phase, the \ac{DT} could compute multiple states corresponding to variations in physical properties such as accelerometer readings or energy consumption, ensuring the \ac{DT} continuously reflects the PA's behavior in a dynamic and precise manner.

When $LP_{Soft.}$ is \texttt{Done}, the \ac{DT} synchronization has been intentionally paused, and the \ac{DT} remains active for accessing stored information. In this phase, $\text{LP}_{PA}$ is empty, $S_{DT}$ is empty, and $H$ contains the history until the time the \ac{DT} is decommissioned denoted as $t_{d}$.

Finally, when $LP_{Soft.}$ is \texttt{Stopped}, the \ac{DT} lifecycle is terminated. $LP_{PA}$ is empty, $S_{DT}$ is empty, and $H$ is also empty, as the \ac{DT} is offline and data is no longer accessible.

By clearly defining and distinguishing between the various phases, the approach enhances the alignment between \ac{DT}s and their physical counterparts, ensuring that each transition and state change is accurately captured and reflected.
The main benefits of this approach include: 
\begin{itemize}
    \item \textit{Enhanced Cyber-Physical Awareness}: a structured lifecycle model ensures that critical transitions of phases and states of the \ac{PA} are precisely tracked and understood, enhancing the overall awareness and responsiveness of the system;
    \item \textit{Better Decision-Making}: with a clear understanding of each phase and its impact, applications and services can make more informed decisions based on accurate and timely information about the current relationship between \ac{PA} and \ac{DT};
    \item \textit{Adaptability}: this structured approach can be adapted to various applications, making it versatile and applicable across different domains.
\end{itemize}

The approach assumes that the $L_{PA}$ is either provided by the \ac{PA} or can be identified by the \ac{DT} model (e.g., through a classification model).
Modeling these phases can be challenging, especially for complex \acp{DT}.
Despite improved cyber-physical awareness, phases may still be incorrectly mapped due to unforeseen external factors.
Additionally, phase tolerance could impact anomaly detection within the DT, necessitating methodological trade-offs to set this tolerance correctly.

Despite these open challenges, the proposed structured lifecycle model represents an advancement in managing \ac{DT} synchronization with their physical counterparts, providing a robust foundation for future developments.

%=======================================================
\section{Modeling Augmentation Functionalities}
%=======================================================

\acp{PA} are typically limited by the nature of their physical characteristics throughout their lifecycle. In contrast, \acp{DT} can capitalize on software-based flexibility to modify, update, and enhance functionality over time.
These enhancements are accessible through \ac{DT} \acp{API} and can be powered by data-driven models possibly enabling adaptive and intelligent behaviors.
This core property of \acp{DT} is known as \emph{augmentation}~\cite{dt-IoT-context-Minerva-2020}.

Examples of DT-enabled augmentation include adding technical features to retrofit devices through external software, such as enhancing interoperability of \ac{IoT} devices via protocol and data format translation. Another example involves improving security and addressing unresolved bugs by mediating interactions with outdated devices that no longer receive updates. Additionally, \acp{DT} can enhance device services by introducing new high-level actions for operations, or by implementing advanced functionalities powered by machine learning and \ac{AI} algorithms, such as forecasting, anomaly detection, and pattern recognition (\Cref{sec:back:dt:ai}).

Despite the acknowledgment of augmentation as a defining feature of \acp{DT},
this property is often defined only conceptually, with implementations tailored to specific vertical cases.
The lack of a reference model for defining \ac{DT} augmentation results in unclear design patterns and potential performance limitations.
Indeed, without a well-defined and measurable boundary for augmentation features---including resource allocation---the \ac{DT} risks failing to achieve its design goal of accurately representing the \ac{PA} with sufficient \emph{fidelity}~\cite{Bellavista_Bicocchi_Fogli_Giannelli_Mamei_Picone_2023}.

%--------------------------------------------
\subsection{Extending the DT Model with Augmentation}
%--------------------------------------------

Recalling the \ac{DT} model structure introduced in \Cref{sec:dte:engineering-dt:abstract-architecture}. 
We here extend it to explicitly include augmentation functionalities.
%
In practical terms, augmentation serves as a bridge transforming raw data into meaningful, actionable outcomes guided by the underlying model.
\acp{DT} may expose additional services and functionalities that enhance the capabilities of the \ac{PA}.
We can represent such functionalities as a set of \emph{augmentation functions} $F$, where $Augm_{i}$ denotes the individual augmentation functions implemented within the DT. Each augmentation function $Augm_i$ may leverage a specific model $m \in M$, to interpret and respond to changes in the DT.

\begin{equation}\label{eq:augm_set}
    F = \{Augm_{1}, Augm_{2}, \dots, Augm_{n}\}\\
\end{equation}

The model in \Cref{eq:5D_dt_model} can hence be extended to: 

\begin{equation}
DT = \langle PI, M, Shad, F, H, DI \rangle
\tag{\ref{eq:5D_dt_model}}
\end{equation}

We distinguish augmentation functions based on two aspects:


\paragraph{Triggering mechanism} (\( \tau \)): defines how and when the function is activated in response to events within the \ac{DT}.
%
\begin{equation}
\tau \in \{ e_{DT}, e_{Aug}, e_{Time} \}  \sigma \in \{S_{aug}, \varnothing\}
\end{equation}
%
\(\tau\) can be an event of three types:
%
\begin{itemize}
\item \( e_{DT} \) represents the output from the shadowing process indicating a new computation of the \ac{DT} state \( S_{DT} \);
\item \( e_{Aug} \) can be generated either as the output of another augmentation function or as a direct invocation by the shadowing process;
\item \( e_{Time} \) refers to temporal triggers and can be used to model periodic execution or specific scheduled tasks.
\end{itemize}
%
Notably, augmentation functions can not respond to triggers coming directly from the \ac{PI} as such $e_{PA}$.
Physical events need to be processed by the shadowing process first, which may in turn select the activation of a relevant augmentation function firing a specific $e_{Aug}$.



\paragraph{State management} (\( \sigma \)): indicates whether the function maintains an internal state that persists over time or is \emph{stateless}.
%
\begin{equation}
\sigma \in \{S_{aug}, \varnothing\}
\end{equation}
%
A stateful function possesses a state \( S_{Aug} \) that evolves and persists over time in alignment with the evolution of the \ac{DT}.  
A stateless function simply reacts to triggers without maintaining any memory of previous invocations.  
Notably, modeling stateful functions allows for the representation of processes that may even run in parallel with the \ac{DT} main control flow, producing new events \( e_{Aug} \) asynchronously.  
Since the function is stateful, it is triggered to start but can persist over time, as its execution is fully encapsulated within the function context. Additionally, the function may receive new triggers over time that feed into its ongoing process.

The output of an augmentation function \( Augm_{i} \) is determined by its inputs and can be expressed as:

\begin{equation}
    Augm_{i}(\tau, m, \sigma, H) =
    \begin{cases}
    e_{Aug} & \text{\textbf{if} $m$ matches}\\
    \varnothing & \text{\textbf{otherwise}}
    \end{cases}
\end{equation}

The triggering event $\tau$ may carry important information for the function execution.
The parameter $m \in M$ represents the model used within the augmentation function, encapsulating the rules, logic, and conditions that define how the function interprets its inputs and determines the output.
The model is crucial for evaluating whether the function should generate an output based on the current state of the function $\sigma$ and context, which is represented by the \ac{DT} history $H$ which includes the latest state $S_{DT}$.

The output \( e_{Aug} \) represents an event generated by the augmentation function, signaling that the conditions defined by the model \( m \) have been met. Conversely, if the conditions are not satisfied (i.e., if \( m \) does not align with the current state or inputs), the output will be \( \varnothing \), indicating that no new event has been produced.  
This approach captures the idea that the execution of an augmentation function does not always result in the generation of a new event. For example, in the case of an anomaly detection model, an event is only triggered when an anomaly is detected with a certain confidence threshold, meaning that no event is generated if the conditions for an anomaly are not met.
Since $e_{Aug} \in \tau$, augmentation functions can also be chained to model processing pipelines triggered by the outputs of previous functions.

%--------------------------------------------
\subsection{Impact on Shadowing}
%--------------------------------------------

The introduction of augmentation functions requires to adapt the shadowing process to handle augmentation events. Indeed, the shadowing process should be capable of receiving relevant augmentation events (e.g., detected anomalies) and use them to update the state of the DT, as illustrated below:

\begin{equation}
    e_{DT}(S_{DT}) = Shad_{DT \rightarrow DT}(M, e_{Aug}, H)
\end{equation}

The same principle applies when a physical event triggers an augmentation function. There is no direct link between physical world events (\( e_{PA} \)) and augmentation functions; 
instead, the decision to trigger an augmentation function based on the receipt of a physical event is mediated by the shadowing process.
The shadowing process analyzes the \( e_{PA} \) and determines whether it is relevant for an augmentation function in $F$ (or multiple functions simultaneously).  If the event is deemed as relevant, the shadowing process generates a trigger by producing an augmentation event (\( e_{Aug} \)), which then serves as the trigger \( \tau \) for one or more augmentation functions.

\begin{equation}
    e_{Aug} = Shad_{PA \rightarrow DT}(M, e_{PA}, H, F)
\end{equation}


Similarly, actions requests that are received by the \ac{DT} ($a_{DT}$) can also trigger augmentation functions.
Again, there is no direct relationship between the action taken on the DT, as represented by its state, and the execution of the augmentation function. Instead, this process also goes through the shadowing process, which matches the request received on the \ac{DI} with the corresponding augmentation function trigger, thus generating an augmentation event that will serve as the trigger for the augmentation function.
This can be used to model a request-response pattern for the execution of augmentation functions such as requesting the predicted next state of the \ac{DT}. In such cases, since the result of the augmentation does not impact the state of the \ac{DT}, the result can be directly routed to the external consumer of the \ac{DT}.

\begin{equation}
    e_{Aug} = Shad_{DT \rightarrow DT}(M, a_{DT}, H, F)
\end{equation}


Despite the shadowing might not be required to validate the result of an augmentation function triggered by an action request, it is still enforced to handle the incoming requests. 
This design aims to decouple the available implementations of the augmentation functions from the actions presented on \ac{DI} and handle this transparently from the consumer perspective.
For example, there might be several augmentation functions implementing a given functionality and it would be up to the shadowing process to determine which one is best suited to respond to a specific request depending on the current context of the \ac{DT} (e.g. the state prediction functionality could be implemented using different specialized models depending on some conditions in the state of the \ac{DT}).

%----------------------------------------------
\subsection{Engineering Augmentation Functions}
%----------------------------------------------

Engineering augmentation functions following the proposed model requires defining
software architectural patterns that can be practically applied to implement them.
Depending on the implementation, it is possible to define \emph{internal} and \emph{external} augmentation.  
This setup allows for the dynamic addition of new functionalities to the \ac{DT}, either through external or internal processing.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/event_driven_augmentation.pdf}
    \caption{Event-Driven architecture of a DT to support and enable effective Augmentation function management and execution.}
    \label{fig:event_driven_augmentation}
\end{figure}

As \Cref{fig:event_driven_augmentation} shows, the core architecture of the \ac{DT} is centered around a shared \texttt{Event Bus}, which serves as the communication backbone, handling events and actions within the \ac{DT} system.
Starting from the \ac{PI}, physical events \( e_{PA} \) are processed by the \texttt{Shadowing Function} ($Shad$) which computes the \ac{DT} State $S_{DT}$ based on the received events and the \texttt{\ac{DT} Models} ($M$), ensuring that the \ac{DT} accurately reflects the current state of the \ac{PA}.
The \texttt{\ac{DT} State} component maintains this state in memory, while the \texttt{History} ($H$) component persists past events and states for reference and analysis.

\texttt{Augmentation Functions} (\( Augm_{i} \)) provide additional capabilities or insights to the \ac{DT}, interacting with \( M \), \( H \), and internal state information \( \sigma \).
The interaction between the \( Shad \) and the \( Augm_{i} \) is entirely event-driven, based on trigger events \( \tau \) and \( e_{Augm} \) flowing out of the augmentation functions with their results.
These results are then sent to the \( Shad \) for further state computation and to the \texttt{Digital Interface} (\( DI \)) to respond to requests for \( Augm_{i} \) from external services and applications.
The \( DI \), mirroring the \ac{PI}, handles outgoing events of the \ac{DT} (e.g., \( e_{DT} \) carrying the new \( S_{DT} \) or its variations) or \( e_{Augm} \) with the results of an augmentation function, and manages incoming actions and requests from digital entities as digital actions \( a_{DT} \).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/augmentation_patterns.pdf}
    \caption{Patterns for augmentation functions execution: internally (left), externally (center), or through a hybrid solution (right).}
    \label{fig:augm_function_event_driven_patterns}
\end{figure}


Based on this event-driven model, we identify three approaches for implementing augmentation functions (\Cref{fig:augm_function_event_driven_patterns}):
\begin{itemize}
    \item \textit{internal} augmentation, where functions are executed within the \ac{DT} process;
    \item \textit{external} augmentation, where functions are executed outside the \ac{DT} process by means of an external service;
    \item \textit{hybrid} augmentation, where functions are executed through a combination of internal and external processes.
\end{itemize}

\subsubsection{Internal Augmentation}
Augmentation functions are executed \textit{internally}, i.e. within the same operating system process as the \ac{DT}, thus sharing the same resources allocated to the \ac{DT} instance (e.g., CPU or GPU computation capabilities).
From an external perspective, these functions are perceived as internal functionalities of the \ac{DT} and only the internal event bus of the \ac{DT} is used.

This approach ensures low latency and tight integration with the \ac{DT} core functionalities, as overhead between the \ac{DT} and the augmentation functions is minimized.
%
For instance, in a manufacturing scenario an internal augmentation function could periodically monitor the operational parameters of a machine physical counterpart.
By running within the \ac{DT} process, this function can quickly react to changes in the machine state, such as detecting abnormal vibrations that might indicate a potential failure (e.g., using signal processing approaches).
The internal event bus would handle the real-time data stream from the machine sensors, feeding the computation of the \ac{DT} state while allowing the augmentation function to promptly process this data and update the \ac{DT}.

\subsubsection{External Augmentation}
Augmentation functions run \textit{externally} as processes entirely separate from the \ac{DT}, possibly without any knowledge of the \ac{DT} internal model.
Since the functions are implemented on external processes, they can use inter-process communication to synchronize and hence they can be distributed across different computing nodes and resources, for a more fine-grained allocation.
%
This translates either in multi-process deployment of the \ac{DT} on the same host, or possibly in fully distributed deployment of the \ac{DT} on a computing cluster.

In the latter scenario, multiple event buses are present. The primary event bus will always be the internal one, while additional external event buses, possibly implemented with different technologies, will be integrated with the internal one. Such integration allows the system to provide triggers, events, and information to the augmentation functions and retrieve their results to synchronize them with the \ac{DT} logic in a completely transparent manner, both for the \ac{PA} and external applications.

A \textit{Synchronization Interface (Sync)} within the \ac{DT} will be responsible for synchronizing the internal and external event buses concerning the events and triggers associated with the augmentation functions. This component must be capable of managing the protocols or formats for mapping between the logic of the internal event bus and the external one.

Since augmentation functions can be external, the state (\( \sigma \)) of an augmentation function will be maintained externally to the \ac{DT}. For internal augmentation functions, \( \sigma \) will be internal to the \ac{DT}, but isolated from other \ac{DT} components and solely dedicated to the augmentation function.

Taking as reference a manufacturing use case, we can exemplify the value of such externalization by considering a \ac{DT} monitoring various machines on the production line, collecting real-time data on operational status and performance.
Augmented features for maintenance prediction and production schedule optimizations could be delegated to an external data analytics service which, using the external event bus, may process the \ac{DT} data with advanced machine learning algorithms and send back results to the \ac{DT} without overloading the \ac{DT} core system.

\paragraph{Hybrid Augmentation}
Augmentation functions run both internally and externally, by combining the strengths of both methods.
A synchronization interface ensures communication between the internal and external event buses.
%
Stateful functions can store their state both externally, for those that require external resources, and internally, as a structural element of the \ac{DT} for internal functions.

This approach is the most flexible and offers significant advantages.
For example, in a smart city application, internal augmentation functions can handle real-time traffic monitoring, benefiting from low latency and immediate response times.
Meanwhile, external augmentation functions can process long-term traffic pattern analysis for strategic urban planning, leveraging powerful external computational resources without burdening the \ac{DT} core system.
%
Such hybrid model provides the flexibility to optimize for both real-time and complex, resource-intensive tasks, ensuring that the \ac{DT} remains efficient and scalable.

%=======================================================
\section{The \acl{WLDT} Framework}
%=======================================================


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/engineering-wldt/wldt_architecture.pdf}
    \caption{WLDT Framework's Software Architecture.}
    \label{fig:wldt_architecture}
\end{figure}


The WLDT framework has been re-designed and extended with key requirements: \textit{Simplicity}, \textit{Extensibility}, \textit{Portability}, and \textit{Microservice Readiness}.
Simplicity ensures easy creation of DT instances using existing modules or custom behaviors for specific scenarios. 
Extensibility allows easy API extension, enabling customization and addition of new features through concurrent module execution.
Portability and microservice readiness ensure that DTs can run on any platform without modifications.
The core engine is lightweight, offering IoT features for developing DT applications as independent microservices.
The main components of the WLDT architecture are illustrated in Figure~\ref{fig:wldt_architecture}, which outlines the three architectural levels: the core library, the DT model, and the implementation of physical and digital interfaces through adapters.
In the WLDT framework, \textit{adapters} -- modular components that implement the PI and DI -- are essential for facilitating communication between \ac{PA}s and DTs, as well as with external digital entities.
Physical Adapters (PAs) interface with the physical world, while Digital Adapters (DAs) manage interactions within the digital environment.

The \textit{Metrics Manager} provides the functionalities for managing and tracking various metrics within DT instances, combining both internal and custom metrics through a flexible and extensible approach.
The \textit{Logger} is designed to facilitate efficient and customizable logging within implemented and deployed DTs, with configurable log levels and versatile output options.
The \textit{Utils \& Commons} component contains a collection of utility classes and common functionalities that can be readily employed across DT implementations, ranging from handling common data structures to providing helpful tools for string manipulation.

%%%
\begin{figure}[th!]
    \setlength{\belowcaptionskip}{-8pt}
    \centering
    \includegraphics[width=\columnwidth]{figures/engineering-wldt/fischer-layout-first-level-dt.pdf}
    \caption{A schematic view of the target industrial scenario.}
    \label{fig:discher-dt-first-level}
\end{figure}
%%%

The \textit{Event Communication Bus} represents the internal Event Bus, designed to support communication between the different components of the DT instance (PI, DTM and DI). It allows for the definition of customized events to model both physical and digital inputs and outputs. Each WLDT component can publish on the shared Event Bus and define an Event Filter to specify which types of events it is interested in managing, associating a specific callback to process different messages.

%%%
\begin{figure*}[th!]
    \setlength{\belowcaptionskip}{-13pt}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/engineering-wldt/DTs-zoom-in.pdf}
    \caption{Internal structure of machine-level DTs with the details of adapters, states and capabilities}
    \label{fig:dt-internal-structure-overview}
\end{figure*}
%%%

The \textit{Digital Twin Engine} defines the multi-threaded engine of the library, allowing the execution and monitoring of multiple DTs and their core components simultaneously.
It orchestrates the different internal modules of the architecture while keeping track of each one, and it serves as the core of the platform, enabling the execution and control of deployed DTs.
Currently, it supports the execution of twins within the same Java process, but the same engine abstraction could extend the framework to support distributed execution through different processes or microservices.

The DT component consists of a modular structure within WLDT, combining core functionalities and capabilities of physical and digital adapters.
Each DT's core module is its \textit{Model}, which is responsible for implementing the twin's behavior over time. This model controls both the \textit{Shadowing Function} and \textit{Augmentation Function(s)}.
The Shadowing Function manages the digitalization process by integrating data from physical adapters and action requests from digital adapters.
It maintains the DT's \textit{State}, organizing properties, events, actions, and relationships as introduced in Section \ref{sec:dt_modelling}.
Augmentation Functions enhance physical capabilities by introducing new properties and actions through intelligent functionalities or dedicated processing (e.g., anomaly detection or data aggregation).
The DT's core is managed by the library to ensure consistency and lifecycle management, adapting to computational phases and interactions with physical and digital realms via adapters. 
Further sections elaborate on these aspects concerning lifecycle structure and adapters.

%%%
\subsection{Cyber-Physical Awareness \& Life-cycle}
\label{sec:dt-lifecycle}

The modeling of DTs involves defining their life cycle, which we have mapped into five states: \textit{UnBound}, \textit{Bound}, \textit{Synchronized}, \textit{OutofSync}, and \textit{Done}.
These states are directly inspired by recent state-of-the-art definitions in the field, particularly from \cite{web_of_dt} and \cite{bellavista2023requirements}, which provide a comprehensive framework for understanding and managing the dynamic evolution of DTs across different stages.

In the \textit{UnBound} state, all internal modules are active but not yet associated with the corresponding \ac{PA}.
The DT transitions to the \textit{Bound} state once the binding procedure is completed, establishing a connection with the \ac{PA} and enabling bidirectional event flow.
In the \textit{Synchronized} state, the DT’s state is aligned with the \ac{PA}, initiating the shadowing process.
The \textit{OutofSync} state indicates errors in the shadowing process, where state alignment or application-generated events cannot be processed.
Finally, the DT reaches the \textit{Done} state when the shadowing process ends but remains active for external requests (e.g., asset history access after its lifecycle).

To transition from \textit{UnBound} to \textit{Bound}, the DT establishes a relationship with the Physical Layer.
For instance, a \ac{PA} using two protocols, $P_1$ and $P_2$, communicates through dedicated adapters.
A key challenge is discovering the \ac{PA}’s capabilities to begin the digitalization process. WLDT addresses this by describing a \ac{PA}’s characteristics, properties and actions through its physical adapters enabling the DT to synchronize with the \ac{PA}.
The DT transitions to \textit{Bound} only when the required adapters are bound and \ac{PA} descriptions are generated, even when multiple \ac{PA}s are associated with a single DT.

To move from \textit{Bound} to \textit{Synchronized}, the DT identifies \ac{PA} capabilities for digitalization and starts the shadowing process (e.g., monitoring a thermostat's target temperature and humidity sensors).
The DT’s Model begins observing these properties through the adapters, which communicate with the \ac{PA} and generate events for property changes.
The DT’s Shadowing function processes these events to compute the new DT state, maintaining synchronization with the \ac{PA}.
Any change in the DT state generates an event, which is exposed to the digital world through the Digital Interface and its Adapters.
These events may also trigger actions on the DT or propagate requests to the \ac{PA} if needed.

%%%
\subsection{Modular and Reusable Digital \& Physical Adapters}
\label{subsec:adapters}

A Physical Adater (PA) in WLDT is responsible for connecting to a specific \ac{PA}. 
Developers can either leverage existing adapters—customized through configuration for context-specific reuse—or create new ones tailored to their specific requirements.
The core responsibilities of a PA include also the generation of a description of the associated \ac{PA} that details the properties, events, actions, and relationships of the \ac{PA}; producing events to reflect changes in the physical state, such as variations in properties, events, and relationships; and handling action requests from the digital realm via the DT shadowing process and the twin's model.
Each PA has an internal dedicated lifecycle within the DT to ensure it can respond to incoming actions appropriately. Adapters are identified by unique IDs, enabling the library to coordinate multiple adapters, adjust logs, and execute functions upon receiving new events. 

Conversely, Digital Adapters (DAs) in WLDT act as the bridge between the DT's core and the external digital world.
Their primary functions include receiving and exposing events related to changes in properties, events, available actions, and relationships from the DT’s core;
communicating the received information to external systems according to the implementation and supported protocols;
and handling incoming digital actions, forwarding them to the core, and ensuring they are validated and processed by the Shadowing Function.
Each DA has an internal life cycle to support its coordination by the WLDT core and has also direct read access to the current twin's State through callbacks or synchronous access, allowing it to navigate all fields of the current digital state and expose it in the digital domain through various protocols according to the application context.


%========================================================
\section{Benefits of the Proposed Approach}
%========================================================

\subsection{Interoperability in a Manufacturing Use Case}
\label{sec:industrial_use_case}
%%%

%%%
\begin{figure*}[t]
    \setlength{\belowcaptionskip}{-13pt}
    \centering
    \includegraphics[width=\textwidth]{figures/dt-interoperability/dt_interoperability-pad_dt_wot.pdf}
    \caption{The transition from physical to digital representation via the \ac{PAD}, the \ac{DT} State, and the \ac{DTD} for external consumers.}
    \label{fig:pad_dt_wot}
\end{figure*}
%%%

The proposed \ac{DT} architectural approach is showcased using the \emph{Fischertechnik Training Factory Industry 4.0}\footnote{Fischertechnik Industry \& Universities: \url{https://www.fischertechnik.de/en/products/industry-and-universities}} Indexed-Line Station, controlled by a Siemens PLC that publishes real-time data via OPC-UA, integrated with Arduino RP2040 boards\footnote{Arduino RP2040: \url{https://docs.arduino.cc/hardware/nano-rp2040-connect/}} for accelerometer data collection and processing and communicating over MQTT\footnote{MQTT Protocol: \url{https://mqtt.org/}}.

\begin{figure}[t]
  \setlength{\belowcaptionskip}{-5pt}
  \centering
  \begin{subfigure}[t]{0.32\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/dt-interoperability/machine_complexity_index.pdf}
      \caption{Machine Digital Twin}
      \label{fig:machine_complexity_index}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/dt-interoperability/station_complexity_index.pdf}
      \caption{Station (Indexed-Line) Digital Twin}
      \label{fig:station_complexity_index}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/dt-interoperability/factory_complexity_index.pdf}
      \caption{Factory Digital Twin}
      \label{fig:factory_complexity_index}
  \end{subfigure}
  \caption{Schematic representation of cyber-physical complexity and its impact on various components.}
  \label{fig:dt_complexity_index}
\end{figure}

The scenario emulates a production line, with a \ac{DT} system deployed to monitor production and measuring the overall efficiency.
The \ac{DT} ecosystem architecture is depicted in Figure \ref{fig:mf_dt_ecosystem}.
Four machine-level \acp{DT} are deployed, each with a \ac{PI} supporting the relevant communication protocols to interact with the different machines, namely through MQTT and OPC-UA\footnote{OPC-UA: \url{https://opcfoundation.org/about/opc-technologies/opc-ua/}}.
These \acp{DT} track machine states, monitor performance, and coordinate production by processing data from the physical system and digital action requests.
An additional department-level \ac{DT} aggregates data from machine-level \acp{DT}, extracting system metrics and exposing department-level actions. 

All the \acp{DT} are built using the \ac{WLDT} open-source framework\footnote{WLDT framework: \url{https://wldt.github.io}}: a Java-based multithreaded stack, facilitating the definition of \ac{DT} and supporting the implementation of shadowing and digitalisation processes.
%Within the framework, we implemented our proposal of physical and digital adapters to validate the approach and facilitate interoperability with a broader range of devices and applications.
The frameworks implements our proposal of physical and digital adapters that we exercise in this example.

\ac{DT} of composed physical assets, such as the \textit{Indexed Line Department} \ac{DT} (see Figure \ref{fig:mf_dt_ecosystem}, top) use lower-level \acp{DT} as data sources.
Notably, the physical and digital adapters that implement MQTT, OPC-UA, and HTTP protocols, along with \ac{WoT} over HTTP for serving \acp{TD}, are reusable across different instances.
These adapters, through configurable parameters, enable effective communication and integration within both digital and physical environments.

Machine-level \acp{DT} provide real-time data on their physical counterparts, such as workpiece presence, operational state
(e.g., idle, working, waiting, or broken),
and performance KPIs like \ac{OEE}\footnote{OEE: \url{https://www.oee.com/}}, calculated from production rate, uptime, and downtime.
%
At the department level, \acp{DT} representing the Indexed Departments assess performance by computing Weighted \ac{OEE}~\cite{OEE-manufacturing-cell-Gamberini-2017,Introduction-to-TPM-total-productive-maintenance-Nakajima-1995}, derived from individual machine \acp{DT}, and track throughput, defined as processed pieces per unit time.
Throughput is determined by monitoring entry and exit events within the department, utilizing relationships captured by the associated \acp{DT}.

Upon startup, each \ac{DT} connects to its data source, publishes the \ac{PAD} of the physical asset, and interacts with the PLC or Arduino to collect and send data, enabling digitalization and interoperability among heterogeneous equipment. 

Figure \ref{fig:pad_dt_wot} schematically illustrates the evolution of data and representation from the physical to the digital realms through the adoption of the proposed \ac{DT} architectural approach, \ac{PAD} integration, and \ac{WoT} \acp{TD} used as \acp{DTD} on the digital side, specifically for the output conveyor of the target production line.
On the physical side, data is structured using OPC-UA with a hierarchical organization, containing telemetry data and action methods mapped to specific data types.
Accelerometer data is transmitted over MQTT on a specific topic with a JSON payload containing axis acceleration values. 

The \acp{PAD} act as an intermediary, translating data from the \ac{PA} into a format suitable for the DT core, decoupling it from the complexity of interacting with the \ac{PA} and enabling discovery, understanding, and utilization of available data and methods.
Using the information from the \acp{PAD}, the \ac{DT} model computes the \ac{DT}'s state with target properties, events, and actions, based on either one-to-one matching of physical characteristics or augmented by combining multiple physical properties into computed \ac{DT} properties or events, such as computing the value of the \ac{OEE} property or triggering anomaly detection events based on accelerometer data. 

Finally, the \ac{DiA} of the \ac{DT} leverages \ac{WoT} \ac{TD} to describe the DT's data and functionalities through a standardized, interoperable, and machine-readable approach.
The \ac{TD} specifies how these can be accessed via protocols and data formats.
This approach enables effective interoperability from the physical to the digital, using a modular and decoupled structure through \acp{DT} in the target industrial use case.

\subsection{Mapping Cyber-Physical Complexity}
\label{sec:mappint_cp_complexity}

In our experimental analysis, we also aim to measure the impact of digitization and responsibility decoupling by comparing the system complexity in scenarios with and without \ac{DT} adoption and also with respect to the different \ac{DT}'s architectural layers.
This allows us to assess the benefits of a modular, structured DT-based approach, particularly in terms of interoperability and heterogeneity management.
To quantify how effectively our approach manages cyber-physical heterogeneity, we adopt the \textit{Cyber-Physical Complexity Index (CP-CI)} proposed in \cite{lippi_dt_causality_learning, LOMBARDO2024107203}.
The CP-CI quantifies the complexity of a cyber-physical application based on:
i) \textit{Required Protocols (p)}: communication protocols needed for device interaction;
ii) \textit{Communication Patterns (c)}: interaction models (e.g., Publish/Subscribe, Request/Response);
iii) \textit{Data Formats (d)}: required data representations or serialization methods;
iv) \textit{Interaction Points (n)}: modules or services involved in data exchange; and
v) \textit{Aggregation Points (a)}: levels of abstraction for physical data (e.g., merging data from multiple machines).
Each criterion is weighted with a \textit{Criteria Importance Factor ($CIF$)} from 1 (low) to 3 (high), indicating its impact on development, deployment, and maintenance.

% The CP-CI reflects the perceived complexity of a cyber-physical application, based on the following criteria: i) \textit{Required Protocols (p)}: number of communication protocols needed to interact with physical devices; ii) \textit{Communication Patterns (c)}: number of interaction models (e.g., Publish/Subscribe, Request/Response); iii) \textit{Data Formats (d)}: number of data representations or serialization methods required; iv) \textit{Interaction Points (n)}: number of distinct modules or services involved in data exchange; and v) \textit{Aggregation Points (a)}: number of composition levels needed to abstract physical information (e.g., merging data from multiple machines in a line). Each criterion is weighted using a \textit{Criteria Importance Factor ($CIF$)}, ranging from 1 (low) to 3 (high), to reflect its impact on development, deployment, and system maintenance.

Focusing on \ac{DT} interoperability, we extended the CP-CI to include both inbound and outbound interfaces, essential for bridging the physical and cyber layers with differing requirements.
This extension enables a more precise assessment of the complexity in managing interoperability across system boundaries.
The CP-CI was applied not only to the entire \ac{DT} but also to its internal layers: Physical Interface (PI), Core, and Digital Interface (DI).
These results were compared to those of a monolithic external application that directly handles business logic and interoperability, bearing the full complexity, highlighting the advantages of the modular \ac{DT} design.
High importance ($CIF=3$) was assigned to managing heterogeneous data formats ($d$) and aggregation points ($a$), as they are crucial for creating interoperable data models.
Medium importance ($CIF=2$) was given to protocol diversity ($p$) and interaction with multiple entities ($n$), which become more significant as systems scale.
Low importance ($CIF=1$) was assigned to multiple communication patterns ($c$), as their concurrent use is standard in distributed applications.

The graphs presented in Figure \ref{fig:dt_complexity_index} display the CP-CI values obtained across different levels of deployment: a single machine, a station in the factory (such as the indexed line, where multiple machines and their respective DTs are managed and composed into a single, unified DT), and the entire factory, which includes 9 machines, 2 composed DTs, and one overarching DT for the entire factory, all deployed following the proposed modular and interoperable approach.
For the machines analyzed, the involved protocols ($p$) and communication patterns ($c$) are both set to 2, as we utilize MQTT and OPC-UA with Pub/Sub and Request/Response interactions.
On the digital side, both MQTT and HTTP are employed.
Regarding data formats ($d$), each machine manages 2 distinct formats, accounting for both PLC data and Arduino accelerometer information.
Furthermore, the interaction points ($n$) and aggregation points ($a$) are both 2, reflecting the aggregation and interaction with two different sub-entities for each machine and its corresponding DT.

The results for single machines (Figure \ref{fig:machine_complexity_index}) highlight the critical role of the PI in the \ac{DT} architecture.
As a decoupling layer, the PI isolates physical-world complexities from the DT core, managing priorities like protocols, data formats, interaction points, aggregation strategies, and communication patterns.
This allows the DT core to focus on uniform data, processed through the interface via PAD descriptions and payload transformations, without dealing with heterogeneous physical entities.
Consequently, the core interacts only with its internal communication pattern and consistent interaction points, regardless of the physical environment’s configuration.
A similar approach is applied to the DI, which is decoupled from physical complexities and adapts to external digital systems, ensuring modularity and reuse.

In contrast, systems without a \ac{DT} must embed all heterogeneity management into a single application, burdening it with business logic, protocol diversity, and data format interoperability.
This increases complexity and reduces scalability, as reflected by the complexity index parameters.

% The results for single machines (reported in Figure \ref{fig:machine_complexity_index}) underscore the strategic importance of the PI within the \ac{DT} architecture. Acting as a decoupling layer, the PI isolates the complexities of the physical world from the DT core. It manages key priorities related to the physical domain, including protocols, data formats, interaction points, aggregation strategies, and communication patterns. This separation allows the DT core to focus on uniform data, processed and normalized through the interface using PAD descriptions and payload transformations, rather than managing heterogeneous physical entities. As a result, the DT core is shielded from the diversity of protocols, working only with its internal communication pattern and a consistent set of interaction points, regardless of the physical environment's configuration. A similar approach applies to the DI, which is decoupled from physical-world complexities and its interaction is limited to the DT core, adapting to external digital systems based on the specific application context. This modular approach enhances the separation of concerns and promotes reuse, allowing the same DT core to be deployed with various Physical Interfaces suited to different operational contexts.

% In contrast, systems that lack a \ac{DT} approach, which bridges and manages the cyber-physical layers, must integrate all interoperability and heterogeneity management into a single application. This places the burden of handling business logic, protocol diversity, and data format interoperability on the application itself, leading to increased complexity and diminished scalability, as reflected in the parameters of the complexity index.

As system architecture scales from individual machines to stations, production lines (Figure \ref{fig:station_complexity_index}), or entire factories (Figure \ref{fig:factory_complexity_index}), protocol diversity stabilizes while the number of interaction points and data formats increases due to the need for managing heterogeneous components.
Our complexity measure shows that systems at the station or factory level experience a significant rise in complexity, driven by interaction points, aggregation nodes, and data format diversity.
These results emphasize the benefits of a modular and interoperable approach.
By encapsulating complexity within DT modules and using interoperable representations, developers can simplify application development.
In contrast, monolithic systems struggle with managing interoperability and system evolution as integration requirements grow.


\subsection{Industrial Use Case}
\label{sec:industrial_use_case}
%To demonstrate the applicability of the proposed lifecycle synchronization model, we conducted an experimental evaluation within a real-world industrial scenario. This evaluation highlights how the alignment between the DT and \ac{PA} lifecycles can be effectively achieved through structured data management and phase synchronization. The industrial case involves mapping the lifecycle phases of physical devices.

An industrial scenario has been considered to apply the proposed approach for DT's lifecycle modeling and characterization. Specifically, scaled industrial stations manufactured by Fischertechnik\footnote{Fischertechnik: \url{https://www.fischertechnik.de/en/products/industry-and-universities}} has been adopted as reference industrial prototyping environment with multiple stations and involved machines. The first station, referred to as the \textit{Multiprocess Station with Oven}, features an oven, a robotic arm for handling operations, and a rotary table equipped with multiple processing stages. The second station, named the \textit{Indexed Line}, comprises four conveyor belts, two of which include mechanical processing operations for the workpieces.

\begin{figure}[t]
    \setlength{\belowcaptionskip}{-13pt}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/dt-lifecycle/multiprocess-station-shcematic-overview.pdf}
    \caption{Multi-process Station schematic overview.}
    \label{fig:multiprocess-station-schematic-overview}
\end{figure}

\begin{figure}
    \setlength{\belowcaptionskip}{-13pt}
    \centering
    \includegraphics[width=\textwidth]{figures/dt-lifecycle/graph_machine_logic.pdf}
    \caption{Machine DT Lifecycle with a focus on the different phases in the Synchronization state.}
    \label{fig:graph-machine-logic}
\end{figure}

\subsection{DT Design \& Development}

The design, development, and deployment of Digital Twins (DTs) for the target use case are motivated by the need for precise monitoring, control, and optimization of physical machinery and stations. By creating digital replicas of the physical systems, we aim to enhance operational efficiency, reduce downtime, and enable predictive maintenance through detailed real-time data analysis and simulation capabilities.

To achieve these objectives, multiple DTs have been designed, developed, and deployed to represent the associated Physical Twins (\ac{PA}s). Specifically, each station was divided into several processing machines, with a DT created for each one. Subsequently, a \textit{composed} DT was developed for each station to aggregate data from the individual machine DTs, enabling a complete digital representation of the station. This allows for the monitoring and management of the combined operations of multiple physical assets working together through the production process. For example, Figure \ref{fig:multiprocess-station-schematic-overview} illustrates the schematic representation of the Multiprocess Station along with the DTs it comprises.

The DTs were implemented using the White Label Digital Twin (WLDT)\footnote{WLDT Library: \url{https://wldt.github.io/}} library~\cite{wldt_picone_2021}. WLDT is a Java implementation of an event-driven DT framework that supports data ingestion through IoT interfaces and message queues. We extended the library to support the proposed lifecycle management approach by implementing both the Physical Twin Lifecycle ($LP_{PA}$) and the Digital Twin Lifecycle ($LP_{DT}$). Each DT features various adapters for external communication, including both physical and digital interfaces. For physical interface management, the DTs controlling individual machines use an OPC-UA\footnote{OPC-UA-Foundation: \url{https://opcfoundation.org/}} adapter, which enables interaction with Siemens S7-1200 PLCs\footnote{Siemens-S7-1200-PLC:\url{https://www.siemens.com/global/en/products/}}. Additionally, each DT includes MQTT\cite{mqtt} Digital and HTTP Digital adapters to expose both its current state and the lifecycle's phases it represents through different protocols and interaction patterns (Pub/Sub and RESTful).

Each machine DT handles the synchronization between the $LP_{PA}$ and the corresponding $LP_{DT}$. Furthermore, an anomaly detection algorithm runs within each DT, enabling it to identify and report issues that the PLC alone might not detect, thus dynamically computing the $LP_{PA}$ when necessary (e.g., when a piece does not reach a reference photocell on the machinery within a predetermined time and an anomaly is consequently detected).

The DTs also calculate the \textit{Overall Equipment Effectiveness} (OEE)\footnote{OEE: \url{https://www.oee.com/}}, a key efficiency metric in the industrial domain that measures the productivity of manufacturing equipment by combining its availability, performance, and quality rates. We compute OEE in our DTs to provide a comprehensive measure of equipment effectiveness, which is critical for identifying areas of improvement and optimizing production processes. This data is exposed for external monitoring.

The composite DT uses an MQTT adapter as its physical interface to collect data and lifecycle phases from the individual machine DTs. It incorporates coordination logic that adjusts machine behavior based on their current phases, ensuring seamless station management. Additionally, the composite DT has its own MQTT Digital and HTTP Digital adapters to expose the station’s overall state externally. The described DT structure is illustrated in Figure \ref{fig:multiprocess-station-dt-structure}.

The DTs of the machines operate based on data produced by their physical counterparts. This data originates from readings of all sensors present on the machinery, along with the states of its actuators. These data are mapped to the DT state, ensuring that at any given moment, the DT reflects the state of the physical object. Leveraging these temporal data, it was possible to model the lifecycle of the physical object within the DT lifecycle.

When the DT lifecycle reaches the Synchronized state, it accurately mirrors the corresponding phase of the physical machinery's operation. The modeled lifecycle, as shown in Figure \ref{fig:graph-machine-logic}, outlines the generic phases defined for each machine. These phases not only reflect the actual operation of the machinery but are also inspired by the reference structure for industrial machine phases defined by ISA-95\footnote{ISA-95:https://www.isa.org/standards-and-publications/isa-standards/isa-standards-committees/isa95}. This alignment ensures that our DTs adhere to recognized industry standards, enhancing their applicability and effectiveness in real-world industrial settings.

To accurately define the phases of the physical machinery, it was essential to specify the transition conditions between these phases by leveraging the raw data and telemetry received from sensors and actuators via OPC-UA from the PLC. The DT processes this data to compute and maintain the synchronized lifecycle, ensuring a precise and real-time reflection of the physical object's operational state. This approach enables the DT to seamlessly transition between phases based on the actual conditions and performance of the machinery, providing an accurate digital representation.

\begin{figure}[t]
    \setlength{\belowcaptionskip}{-13pt}
    \centering
    \includegraphics[width=\columnwidth]{figures/dt-lifecycle/mps_dt_structure_2.pdf}
    \caption{Multi-process Station Digital Twin overview.}
    \label{fig:multiprocess-station-dt-structure}
\end{figure}

\subsection{Lifecycle Awareness \& Coordination}

In an industrial setting, the digital representation of a station is often a hierarchical composition of individual machines, each represented by its own DT. This structure enables intelligent coordination not only between machines within the same station but also across different stations. For example, coordination between the \textit{Multiprocess Station} and the previously mentioned \textit{Indexed Line} becomes more manageable compared to a direct access to low level raw data of the different physical machines. Incorporating the PA lifecycle into the DT lifecycle simplifies coordination by establishing consistent lifecycle phases across machines.
Without this approach, coordination among multiple DTs required using low-level physical information and telemetry, making the process more complex and less intuitive. Now, as illustrated by Algorithm \ref{code:soft-stop-logic}, high-level control logic can be implemented directly leveraging information encapsulated by $LP_{DT}$ mapping in real-time the phase and the associated transitions of the associated physical counterparts $LP_{PA}$. Specifically, the algorithm outlines the steps the composite DT can take to halt production on the multiprocess station. By leveraging this additional layer of abstraction, the composite DT no longer relies on raw sensor data for control operations, enabling a more declarative and effective coordination process, and distributing responsibilities across different DTs.

The composite DT leverages the PA lifecycle phases of individual machines to define the overall lifecycle of the entire multiprocess station. This provides a comprehensive overview of each station's operational behavior, enabling more effective and intuitive system management.

\note{FIX ALGORITHM}
% \begin{algorithm}
% \caption{Multiple DTs Stopping Procedure}
% \begin{algorithmic}
% \State \texttt{invoke(OvenDT, STOP)}
% \While{ \texttt{OvenDT.phase} \textbf{is not} \texttt{STOPPED} }
%     \State \texttt{wait(OvenDT.phase} \textbf{is not} \texttt{WORKING)}
% \EndWhile
% %\vspace{1em}
% %\State \textbf{publish} event ``change-vacuum-gripper-cycle" with value \texttt{false}
% \State \texttt{invoke(GripperDT, STOP)}
% \While{ \texttt{GripperDT.phase} \textbf{is not} \texttt{STOPPED}}
%     %\State \textbf{log} waiting for end of vacuum-gripper \texttt{WORKING} state
%     \State \texttt{wait(GripperDT.phase} \textbf{is not} \texttt{WORKING)}
% \EndWhile
% %\vspace{1em}
% %\State \textbf{publish} event ``change-turntable-cycle" with value \texttt{false}
% \State \texttt{invoke(TurntableDT, STOP)}
% \While{ \texttt{TurntableDT.phase} \textbf{is not} \texttt{STOPPED} }
%     %\State \textbf{log} waiting for end of turntable \texttt{WORKING} state
%     \State \texttt{wait(TurntableDT.phase} \textbf{is not} \texttt{WORKING)}
% \EndWhile
% %\vspace{1em}
% \State \texttt{invoke(OutputConveyorDT, STOP)}
% \end{algorithmic}
% \label{code:soft-stop-logic}
% \end{algorithm}



\subsection{Industrial Digital Twins with WLDT}
\label{sec:industrial-dt-wldt}

The proposed DT architectural approach has been initially applied and validated in a realistic industrial environment: a microfactory which serves as an experimental platform for DTs and industrial ecosystems. The industrial environment has been strategically chosen since it represents one of the most challenging application scenarios for DTs, characterized by stringent requirements in terms of integration with heterogeneous systems, and the management of fragmented data through multiple stakeholders. 

However, it is crucial to emphasize that the proposed approach is inherently flexible and designed to be applicable in a wide spectrum of other  contexts. While the initial validation focused on the industrial domain we foster the application of our framework in other domains and use cases, such as Smart Cities \cite{10.1145/3570361.3614070} and Health \cite{9325551} where the concepts of DTs can bring significant value. The modularity and scalability of our solution facilitate its adoption in various types of applications, and future work may explore in detail its effectiveness in these additional contexts.

Specifically, the validation in the industrial domain was carried out using the \emph{Fischertechnik Training Factory Industry 4.0}\footnote{Fischertechnik Industry \& Universities: \url{https://www.fischertechnik.de/en/products/industry-and-universities}} Indexed-Line Station, featuring the \emph{Multiprocess Station with Oven} module.
This microfactory setup provides a controlled yet dynamic environment for testing and refining DT technologies in a realistic industrial context.

%%%
\begin{figure*}
    \setlength{\belowcaptionskip}{-13pt}
    \centering
    \includegraphics[width=0.92\textwidth]{figures/engineering-wldt/Fischer-relationships.pdf}
    \caption{DTs relationships with both those existing in the physical space as well those belonging to the digital composition.}
    \label{fig:dt-assets-relationships-overview}
\end{figure*}
%%%

As shown in Figure~\ref{fig:discher-dt-first-level}, the module consists of five machines arranged in a flow shop layout: three material handling units (vacuum gripper carrier, turntable, and conveyor) and two transformation stations (oven and saw station).
Each machine is equipped with sensors (light barriers and limit switches) and actuators (with two or three operational states), all managed by a 24V industry-grade digital board.
A Raspberry Pi-based soft-PLC, connected to a 24V Digital I/O expansion board, controls the system and coordinates machine operations via sensors and actuators. Real-time process data is published to an MQTT broker DTs can subscribe to to receive data.

The digitalization goal is to monitor the state, track production performance and power consumption, and regulate the production speed of each machine through a DT.
Additionally, to compute the overall production performance and manage external action requests, and to track the overall power consumption, two department-level DTs are created, matching the two different modeling goals.
Each machine-level DT processes incoming data on properties and events to generate insights such as production and energy KPIs, while also managing action requests to regulate their individual speed.
Relationships between DTs are crucial, as they represent connections between physical assets, which can be used by external applications and other DTs, facilitating \emph{compositions} to form higher-level DTs.

% The digitalization of the department begins with the machine layer, using DT compositions to create higher-level DTs. To capture different aspects of the department (e.g., overall production performance and energy consumption), machine-level DTs can be integrated into two distinct department-level compositions, each specializing in one of the targeted aspects.

% This specialization is achieved by selecting specific information in the DT configuration and using the department DT model to manipulate, augment, and output relevant information. Department-level DTs, structured similarly to machine-level DTs, handle action requests through their internal models, just like the lower-level DTs. Relationships maintain their importance, representing actual connections between the department DT (whether specialized in performance or energy tracking) and other DTs within the system.

%%%

\subsection{Machines Digital Twins}
\label{subsec:dt-machine-level} 

The digitalization process creates five machine-level DTs, one for each machine in the factory (Figure~\ref{fig:discher-dt-first-level}). These DTs each interact via an MQTT broker through the PI, are equipped with an internal model implemented by the Shadowing Function, and have a Digital Interface to share data.
The DT ecosystem digitalizes the physical system without replacing the PLC logic, allowing for the independent operation of the PLC if the DT ecosystem fails.
DTs are deployed on edge nodes for monitoring and control without disrupting core operations.

Each DT connects to the broker through MQTT Physical Adapters, which manage subscriptions to relevant topics and publish the description of PA. The internal model of each DT calculates metrics like OEE (based on production rate, uptime, downtime, and quality) and tracks power consumption. Relationships between machines and DTs (e.g., \emph{previous}, \emph{following-machines}, \emph{is-part-of}) are defined at startup and remain static during the experiment.
Machine DT states are exposed via MQTT and HTTP Digital Adapters, enabling flexible interaction patterns and action requests through HTTP.

%%%
\begin{figure*}
    \setlength{\belowcaptionskip}{-13pt}
    \centering
    \includegraphics[width=\textwidth]{figures/engineering-wldt/experimental_results.pdf}
    \caption{Experimental Results: CPU usage percentage [\%], Memory usage [Mib], Network usage [KBit/sec] and OEE [\%]}
    \label{fig:exp-results}
\end{figure*}
%%%

%%%

\subsection{Department Digital Twins}
\label{subsec:dt-department-level}

The identified department DTs model and monitor KPIs and energy metrics based on the machine DTs structure. The Department KPI DT focuses on tracking performance, including weighted OEE and throughput. Weighted OEE is calculated using OEEs received from each machine DT~\cite{OEE-manufacturing-cell-Gamberini-2017}.
Throughput, defined as the number of processed pieces per time unit in production systems, is tracked by monitoring entry and exit events of pieces processed by machines within the department, facilitated by relationships in the DT ecosystem.

The Department-KPI DT tracks piece processing start and completion, calculates throughput, and monitors total pieces processed.
Relationships between lower and upper-level DTs are shown in Figure~\ref{fig:dt-assets-relationships-overview}.
The Department DT accepts action requests via its HTTP digital adapter, adjusting production rates through its Shadowing Function, which cascades actions to underlying DTs and physical assets.
The Department-Energy DT monitors energy consumption, aggregating data from machine-level DTs, and processes it through an internal model to derive target metrics.
It also uses an HTTP digital adapter for external interaction.
Department-level DTs use MQTT-based physical adapters to ensure consistent communication and enable external applications to coordinate actions across the underlying DT instances.

% The Department-KPI DT can track the start and completion of piece processing, calculate throughput based on time per piece, and total pieces processed. Relationships between lower-level and upper-level DTs are illustrated in Figure~\ref{fig:dt-assets-relationships-overview}. The Department DT accepts action requests via its HTTP digital adapter, supporting operations to adjust production rates. Speed changes are processed through a simple algorithm in the Department DT's Shadowing Function, cascading actions to underlying DTs and associated physical assets. The Department Energy DT monitors overall energy consumption, aggregating data from machine-level DTs regardless of whether it's specific energy consumption or disaggregated voltage and current data. Its internal model handles this heterogeneity to derive target metrics. The Department Energy DT also utilizes an HTTP digital adapter for external interaction, providing access to its internal state information. Department DTs utilize MQTT physical adapters within their physical interface, ensuring consistent implementation of composed DTs. They enable external applications to interact with the department through HTTP digital adapters, managing actions across underlying DTs based on the department's internal model and behavior.


%%%
% \begin{figure*}
%     \centering
%     \setlength{\belowcaptionskip}{-13pt}
%     \includegraphics[width=\textwidth]{images/masa_dt_result.png}
%     \caption{Experimental evaluation of the MASA DT deployment with CPU\%, Memory usage\% and the cumulative Network I/O.}
%     \label{fig:masa_dt_results}
% \end{figure*}
%%%

\subsection{Experimental Evaluation}
\label{ssec:masa_exp_evaluation}

To validate the WLDT framework, we implemented the DTs for the Multiprocess Station with Oven, containerized using Docker, and deployed on an Edge node with an Intel i7 2.4GHz processor and 32 GB of RAM. We monitored three key metrics for each DT: CPU usage (percentage, with each container using one core), memory consumption (in MiB), and network traffic (cumulative inbound and outbound in Kbit/sec).
Additionally, we calculated the OEE computed by each DT to assess the performance of both individual machines and the station. Figure \ref{fig:exp-results} shows the results of this experiment. The total duration was 10 minutes (600 seconds), as shown on the x-axis of each chart. The graphs illustrate CPU usage, memory usage, network traffic, and the computed OEE (from left to right). Analyzing CPU usage, we observe that there were no significant spikes during the production cycle. Even during the most computationally demanding moments, CPU utilization remained around 30\%, indicating a stable and efficient execution.
The memory usage chart shows a similarly stable pattern, with DT instances settling around an average usage of 300 MiB, demonstrating consistent memory behavior over time.
As for network traffic, values remained low—typically around a few tens of Kbit/sec.
Noticeable traffic peaks correspond to active processing phases, where machines and their respective DTs exchange larger volumes of data.
Finally, the OEE graph highlights the performance of industrial machines computed by each DT. The Oven, which is the first machine in the station and requires the longest processing time, shows the highest OEE, as it remains under constant workload.
Conversely, the output conveyor exhibits a lower OEE, as it receives fewer parts to process due to the upstream bottleneck created by the Oven. This experimental phase aimed to demonstrate the effectiveness of WLDT-based DTs in an industrial setting.
The collected metrics confirm that DT instances can operate efficiently on standard commercial Edge nodes, handling CPU, memory, and network constraints while also supporting domain-specific computations such as OEE. These results validate the potential of the WLDT framework for practical DT development and deployment in real-world industrial scenarios.