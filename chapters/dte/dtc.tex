This chapter investigates \ref{rq:3} by exploring the challenges in the operational management of \acp{DTE} that involve the management of multiple \acp{DT} from a deployment and runtime perspective.
%
Several \acp{DT} need to be orchestrated on a cyber-physical computing infrastructure, ensuring that \acp{DT} are able to interact with the \acp{PA} they represent and maintain the target level of performance and synchronization. 
%
This problem is further exacerbated when considering that \ac{DTE} can involve \acp{DT} developed for different platforms, and computing infrastructures that span the whole \emph{edge-cloud continuum}. 

Motivated by these challenges, this chapter introduces the concept of a \emph{\ac{DTC}}, a middleware that aims to facilitate the operational management of \acp{DTE} by providing a unified interface to manage the deployment and runtime of \acp{DT} across heterogeneous \ac{DT} platforms and computing infrastructures.
%
This proposal complements the \ac{HWoDT} approach presented in Chapter~\ref{chap:dte:hwodt}, which focuses on the interaction with \ac{DTE}, rather than their operational management. 

The chapter analyzes the challenges in the operational management of \acp{DTE}, and presents a proposal for an architecture of the \ac{DTC} middleware. The functionalities are supported by a set of \emph{descriptions} that can inform the \ac{DTC} about the characteristics of the \acp{DT} and \acp{PA} involved in the \ac{DTE} and support stakeholders in having a clear understanding of the system state at any stage.


%=======================================================
\section{Operational Challenges}
%=======================================================

An orthogonal aspect of complexity when developing a \ac{DT} is understanding its non-functional requirements that allow for considering the \ac{DT} effectively synchronized.
%
The concept of \emph{entanglement}~\cite{dt-IoT-context-Minerva-2020} has been used to define the level of synchronization of the \ac{DT} with the \ac{PA} and can be measured with metrics that include network latency and computation time on both the physical device and the computing node that is running the \ac{DT}~\cite{bellavista2024odte}.

In recent years, with the increased availability of computing resources at the edge of the network, the concept of edge-cloud continuum has emerged as a paradigm to distribute computation across a variety of computing nodes, obtaining benefits in terms of latency and optimizing the overall workload of an \ac{IoT} system~\cite{Rosendo_Costan_Valduriez_Antoniu_2022}.
%
Like other \ac{IoT} system components, \acp{DT} can also be deployed within the continuum~\cite{Bellavista_Bicocchi_Fogli_Giannelli_Mamei_Picone_2024} with trade-offs in terms of entanglement, cost and resource availability.
%
The advantages of this approach, though, are offset by additional complexity in the operational management of the software system.
When it comes to \acp{DT}, this means choosing the suitable computing node to deploy the \ac{DT} software, to ensure the \ac{QoS} requirements are matched at runtime.
This can be challenging due to the cyber-physical nature of \acp{DT}.
Additionally, when several \acp{DT} are employed as parts of an ecosystem sharing the same computing infrastructure, it should be possible for operators to easily navigate the continuum and observe performance metrics to understand the potential impact of deploying new \acp{DT} in the system and eventually reconfigure it to balance the overall workload.

On the development side, the heterogeneity of \ac{DT} platforms poses additional challenges as they may constrain the ability to deploy a \ac{DT} across the edge-cloud continuum.
%
Indeed, while open-source platforms can be adapted to run on a generic infrastructure, proprietary ones are usually Platform as a Service solutions, bound to the cloud infrastructure. 
%
Developers tasked with implementing a \ac{DT} must hence evaluate the suitability of a given \ac{DT} platform, taking into consideration both the functional (i.e., meta-model, features, services) and non-functional (i.e., latency, computing power, privacy) requirements.

Since these requirements may vary significantly depending on the \ac{PA} being modeled it is realistic to assume that a complex \ac{DT}-based system may include \acp{DT} developed on different platforms.
%
This has surfaced challenges in the management of the development and deployment of \ac{DT}-based systems which may need to guarantee the \ac{QoS} requirements of \acp{DT} developed with heterogeneous technologies and platforms while managing a shared pool of heterogeneous computing resources.

The proposal of the \ac{DTC} aims to address the following goals:
\begin{itemize}
    \item \textbf{reducing deployment complexity} of a \ac{DT}-based system integrating \acp{DT} developed for different platforms sharing the same computing infrastructure.
    \item \textbf{facilitating the interaction of different stakeholders} with a \ac{DT}-based system enabling easy access of relevant information over the development-to-deployment lifecycle of \acp{DT}.
\end{itemize}

%========================================================
\section{Phases and Stakeholders}
%========================================================


The process of developing and managing a \ac{DTE} involves several critical phases, each essential for the effective development, discovery, selection, deployment, and operation of \acp{DT}.
%
These phases abstract the development-to-deployment lifecycle of \acp{DT} in a structured approach which is independent of the target application domain or specific technological choices.

Identifying these phases helps shape the requirements for the \ac{DTC} and understand the roles of the stakeholders that are involved in each phase.
%
\Cref{fig:dtc-phases} summarizes the main phases and the stakeholders involved in the development and management of \acp{DT} in a \ac{DTC} that are described in the following.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{figures/dtc/dt-phases-stakeholders-v2.pdf}
    \caption{Stakeholders involved in different phases of the development and management of \acp{DT} in an ecosystem.}
    \label{fig:dtc-phases}
\end{figure}


\paragraph{DT Software Capabilities Discovery \& Selection}
As a starting point, every DT system is originating from the needs of a cyber-physical context.
%
This usually involves one or several \emph{\ac{PA} Owner} who are the stakeholders that possess the knowledge about the \acp{PA} and the goal that requires \acp{PA} to be modeled and digitalized~\cite{michael2024software}.
The \emph{\ac{PA} Owner} is also in charge of granting access to the \ac{PA} data and sensors. 
%
The first phase hence focuses on identifying DT capabilities that align with specific application and use case requirements.
%
This phase involves defining the functionalities of the DT, associating it with the corresponding physical asset category, specifying communication protocols, and detailing its properties, events, relationships, and available actions. 
%
Notably, an asset might have multiple owners.
For instance, an industrial machine is produced by a manufacturer and acquired by a company to employ it in one of its facilities.
The same machine is then owned simultaneously by the manufacturer who may have control of telemetry data and be interested in monitoring the machine to offer predictive maintenance services, by the company who might want to keep track of all the machines across facilities and by the facility manager who might monitor the state of operational of the machine in the specific production line it is employed. 
All the owners may have access to different data and model the same asset with different target goals.


\paragraph{DT Platform Selection and Development}

A \emph{\ac{PA} Owner} may commission a \emph{DT Developer} to implement the DT for the PA. The developer will implement a DT using a target technological stack, that depends on their expertise and the availability of the computing infrastructure that is available for the deployment of the DT.
%
In this phase it is necessary to ensure that the chosen DT requirements can be effectively implemented on a specific platform.
%
This phase involves defining platform-specific configurations, including implementation details, required configuration parameters, and technical requirements.
%
Notably, the \emph{\ac{PA} Owner} is usually the (virtual) owner of the computing infrastructure on which the DT will be eventually deployed. When the DT is developed as commissioned by the \emph{\ac{PA} Owner} this may influence the technological choices of the \emph{\ac{DT} Developer}.
%
A possible future may envision \acp{DT} implementations for specific kinds of assets made available to owners of such assets either publicly or commercially.
These would be implemented either by manufacturers or a community of \emph{\ac{DT} Developers}.
Such reusable \acp{DT} would then be implemented with a given technological stack and need to be deployed on the available resources of the \emph{\ac{PA} Owners}, in order to be configured and connected to the locally available \acp{PA}. 
Similarly, such reusable \acp{DT} could be offered as-a-service and made available only as instances deployed on commercial platforms. This adds a further layer of complexity to the management of the computing infrastructure running the possibly different \acp{DT} in an organization.


\paragraph{Digital Twin Deployment}

Once a DT is developed (from scratch or reusing existing implementations) the next goal is to run it on a computing infrastructure.
This step involves a \emph{DT Operator} who have knowledge about the \ac{DT} requirements and can configure the computing infrastructure to make sure that the DT is deployed correctly and gets access to the \ac{PA} data streams (this can be the same person as the \emph{DT Developer} in a typical Dev-Ops fashion).
%
Deploying a DT can be a challenging task, involving several steps that depend on the complexity of the deployment infrastructure and the requirements of the DT.
%
This phase is crucial to ensuring that all deployment conditions are met, including  availability of deployable artifacts, and correct setup of communication configurations, enabling interaction with both physical and digital entities.
%
In a computing continuum, a DT may be deployed at different levels either on the edge, fog or cloud depending on the admissible trade-offs between network latency and computing power requirements. Notably, \acp{DT} might move across this continuum for a variety of reasons, either being connectivity requirements with the \ac{PA} (and thus move horizontally in the continuum), or drops in the quality of service that require either scaling to the cloud or moving closer to the edge (and thus moving vertically in the continuum). Finally, the same \ac{PA} could be connected to different \acp{DT} replicas, serving different use cases~\cite{dt-IoT-context-Minerva-2020}.
%
Ideally, the DT Operator can delegate a request to a \emph{\ac{DTC} Manager} who has access to data about \emph{other} \acp{DT} running on the same infrastructure and hence deploy the \ac{DT} in order to use resources effectively.
%
While a \emph{\ac{DT} Operator} is fundamentally interested in monitoring the activity and managing one or more \acp{DT}, the \ac{DTC} Manager is instead interested in monitoring the activity and managing the computing infrastructure on which \acp{DT} are being deployed.

\paragraph{Running Digital Twin}

The last phase starts once the \ac{DT} is up and running. In this phase, the DT must be identified, described, and reachable by \emph{consumers}.
%
This phase is critical for maintaining an accurate and up-to-date representation of the DT instance, managing its interaction protocols, handling lifecycle transitions, and enabling real-time state monitoring and interaction with the DT.
%
The ability to monitor information about running \acp{DT} is essential for ensuring synchronization with physical assets, supporting advanced digitalization processes, and facilitating interoperability within complex cyber-physical systems.
%
In this phase, generic \emph{\ac{DT} Consumers} may interact with the deployed \ac{DT} instance. 
%
Such consumers can either be human users or other systems and applications, leveraging the DT features for their purposes.
%
The \emph{\ac{DT} Consumers} are inherently also \ac{DTC} consumers, as they may interact with the \ac{DTC} to discover which \acp{DT} of which \acp{PA} are available, where to find them and how to interact with them.


\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/dtc/dtc-requirements_v2.pdf}
    \caption{Requirements associated to each phase of the \ac{DT} development-to-deployment process.}
    \label{fig:dtc-requirements}
\end{figure}

\Cref{fig:dtc-requirements} summarizes the \emph{information} requirements for managing \acp{DT} across these phases, namely what kind of information is necessary to support stakeholders in each phase, and ensure that the phase can run successfully.
%
These requirements guide the design of \emph{descriptions} that can be used to inform the \ac{DTC} about the characteristics of the \acp{DT} to deploy and manage, in a platform-agnostic way.


%=======================================================
\section{Key Elements and Descriptions}
%=======================================================

To address the challenges in the operational management of \acp{DTE}, the \ac{DTC} relies on a set of key elements and their structured descriptions that provide a structured way to represent and manage the various components involved in a \ac{DTE}.

\begin{table}
    \centering
    \small
    \begin{tabular}{p{2cm} p{8.5cm} p{2cm}}
    \toprule
    \textbf{Entity} & \textbf{Description} & \textbf{Req.} \\
    \hline
    \textbf{Physical Asset Schema (PAS)} & Defines the abstract capabilities and interaction patterns of a specific type of industrial asset (e.g., robotic arm, conveyor system). Provides essential metadata for communication protocols, API definitions, and interface configurations without specifying instance-specific values like IP addresses or credentials. & R1, R2, R3 \\ \hline
    \textbf{Physical Asset Instance (PAI)} & Represents a deployed instance of an asset as defined by the PAS. Includes instance-specific configurations like network settings, authentication credentials, and protocol-specific settings. Facilitates the runtime communication between the DT and the physical asset. & R1, R2, R3 \\ \hline
    \textbf{Digital Twin Schema (DTS)} & Defines the capabilities, behaviors, and structural components of a Digital Twin, linking the digital representation to its corresponding physical asset. Specifies properties, events, relationships, actions, and fidelity between the physical and digital counterpart. & R1, R2, R3 \\ \hline
    \textbf{Digital Twin Package (DTP)} & Defines the platform-specific implementation of the DTS, including code, configuration parameters, and dependencies required for deployment. Ensures the fidelity and communication requirements of the DTS are met during deployment. & R6, R7, R8 \\ \hline
    \textbf{Digital Twin \-  Instance (DTI)} & Represents a running instance of the Digital Twin, including metadata for orchestration, monitoring, and lifecycle management. Tracks software lifecycle states and exposes relevant runtime metrics. Ensures the connection between the DTC and the deployed DT instance. & R9, R10, R11, R12 \\
    \hline
    \textbf{Runtime Platform (RP)} & Describes the underlying computing infrastructure and environment where the Digital Twin is deployed. This includes details about the hardware, software, and network configurations that support the DT's operation. & R4, R5 \\
    \bottomrule
    \end{tabular}
    \caption{Overview of DTC Entities: Characteristics, Responsibilities, and Associated Requirements}
    \label{table:dtc_descriptions_requirements}
\end{table}


At the core of this descriptive framework is the \textit{Physical Asset Schema} (PAS), which serves to uniquely define a specific \textit{type} of industrial asset---such as a robotic arm, conveyor system, or CNC machine.
%
The PAS encapsulates essential metadata that describes how assets of this category are expected to interact within a digital environment. This includes supported communication protocols (e.g., MQTT, OPC UA, HTTP), standardized API definitions, and more broadly, the configuration of interfaces and interaction models required for monitoring, control, and integration across systems. It is important to note that the PAS does not contain instance-specific values such as IP addresses, port numbers, or credentials. Instead, it provides a structured description of the expected capabilities and modes of interaction for devices of a given type.
%
This is similar to the concept of \emph{Thing Model} in the context of the \ac{WoT}~\cite{wot-td} which only focuses on capabilities and abstracts completely from protocols and implementation details. The PAS, instead, includes this information, as it is more closely related to the physical devices and not on their virtual \emph{Thing} representation. 

This schema is essential for the \ac{DTC} to understand how different \ac{PA} conform to specific interaction patterns and how to configure communication with them. 

Building upon the PAS, the \textit{Physical Asset Instance} (PAI) represents a deployed instance of an asset as defined by the PAS.
%
While the PAS defines the abstract capabilities and expected interaction patterns of a category of assets, the PAI provides the concrete, instance-specific configuration required for operational integration. This includes network information (such as IP addresses and ports), authentication credentials, and protocol-specific settings like MQTT topics or OPC UA node identifiers.
%
These configurations are essential to enable runtime communication between the DT and the physical asset, supporting dynamic discovery, secure interfacing. As such, the PAI forms the operational bridge that links the abstract asset description to real-world deployment contexts, ensuring that the DT can monitor and interact with its physical counterpart in a precise and reliable manner.
%
The \ac{WoT} \ac{TD} can be used for this scope as it includes the necessary information to describe how to connect to a specific device instance.

Following the same principles and responsibilities of the PAS, the \textit{Digital Twin Schema} (DTS) defines the capabilities and behaviors of a DT independently of any concrete implementation.
%
It establishes a reference to the corresponding \ac{PA} category, ensuring semantic linkage between the physical and digital representations. The schema outlines the structural components of the DT, such as its properties, observable events, defined relationships, and available actions. It also specifies the target \emph{fidelity} of the DT, capturing the mirrored functionalities of the physical counterpart within a given context and under specific application goals.

The \textit{Digital Twin Package} (DTP) represents instead the software implementation of a given schema on a target \ac{DT} platform.
%
The package includes all platform-specific implementation artifacts required for deployment and execution.
%
It incorporates the executable code, configuration parameters, and platform dependencies, ensuring compliance with the DTS's fidelity and communication requirements.
%
It also provides detailed bindings for the declared communication protocols---e.g., how MQTT is used to interface with physical assets and how HTTP APIs enable interaction with external digital services.

When a package is deployed within the DTC, it gives rise to a \textit{Digital Twin Instance} (DTI).
%
This instance represents the active, running version of the DT within the operational infrastructure and includes all metadata necessary for orchestration, monitoring, and software lifecycle management.
The DTI maintains references to its originating schema and the platform on which it is deployed, along with essential access information such as IP addresses, ports, and authentication tokens. It also exposes relevant runtime metrics—including CPU, memory, and network utilization—as well as any endpoints required to access its services and functionalities.
%
In addition, the DTI tracks and exposes the \ac{DT} connectivity information tracking whether the \ac{DT} is connected to the \ac{PA}. These lifecycle states pertain strictly to the operational status of the deployed software component and are independent of the internal logic or functional behavior of the DT itself (differently from the synchronization lifecycle discussed in \Cref{sec:dte:engineering-dt:dt-lifecycle}).
%
Functional state and domain-specific behavior remain under the responsibility of the DT and are communicated through its defined interfaces in the digital space (e.g., through the \ac{DTKG} if considering a \ac{HWoDT} ecosystem \Cref{chap:dte:hwodt})

With reference to the requirements identified in \Cref{fig:dtc-requirements}, \Cref{table:dtc_descriptions_requirements} summarizes the key elements and their associated responsibilities in supporting the operational management of \acp{DTE}.
%
The integration of the PAS, PAI, and DTS effectively addresses the core requirements \textbf{R1.Capabilities}, \textbf{R2.Asset Linkage}, and \textbf{R3.Communication}, which are focused on the representation of features of the \acp{PA} and \acp{DT}, the relationship that links a \ac{DT} to a specific \ac{PA}, and the necessary communication protocols.
%
These descriptions support the \emph{DT Software Capabilities Discovery \& Selection} phase, enabling stakeholders to explore available \acp{PA} and their corresponding DT types based on defined capabilities and interaction patterns.
%
Additionally, the DTP supports requirements \textbf{R6.Computing Node Requirements}, \textbf{R7.Artifacts}, and \textbf{R8.Connection Configuration}, which pertain to platform-specific aspects to support the creation of DT instances within the DTC and can hence support the \emph{Digital Twin Deployment} phase.

The DTI is responsible for addressing the requirements \textbf{R9.Identification}, \textbf{R10.Instance\- Description}, \textbf{R11.Interaction\- Information}, and \textbf{R12.Life\-cycle}, which in the \emph{Running Digital Twin} phase support stakeholders in the identification of the DT instance, the discovery of its status and interaction descriptions, as well as its connectivity with the \ac{PA} during operation.


\textbf{R4.Platform Description} and \textbf{R5.Configuration} are supported by the \textit{Runtime Platform} (RP) description.
%
Differently from the other descriptions, these are tied to the computing infrastructure and the target platforms on which the \ac{DT} can be deployed. 
%
\textbf{R4}, includes platform-dependent attributes such as the platform type, supported communication protocols, and data formats, enabling the DTC to match DTPs to the correct platform.
%
Additionally, \textbf{R5} plays a crucial role in establishing the necessary parameters and operational requirements specific to the platform and implementation. Proper configuration ensures that the DT functions optimally by aligning its settings with platform constraints, communication protocols, and performance expectations.  

Together, these components support stakeholders to seamlessly identify and discover managed \acp{PA}, understand their capabilities, and determine the availability of corresponding \ac{DT} packages for deployment. This structured approach ensures that the DTC facilitates efficient asset discovery, aligns with system interoperability needs, and supports dynamic integration across diverse environments. 
%
Figure \Cref{fig:dtc-descriptions} visually summarizes the key elements and their descriptions within the context of the DTC, illustrating how they interrelate to support the operational management of \acp{DTE}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/dtc/dt_entities_platform_interactions.pdf}
    \caption{Key elements and their descriptions depicted within a schema of the \ac{DTC}. \acp{PA} instances (bottom) are described by schemas and each have a \ac{DT} instance mirroring them, running on possibly different platforms depending on which DT package among the ones implementing the correct DT Schema (top) is used to deploy the \ac{DT}.}
    \label{fig:dtc-descriptions}
\end{figure}

%=======================================================
\section{Architecture of the \acl{DTC}}
%=======================================================

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{figures/dtc/architecture_new_v3.pdf}
    \caption{Functional Architecture of the \ac{DTC} with the main components and the interactions with stakeholders. On the left, DTC nodes abstracting the interaction with physical computing resources. On top of them, DT packages belonging to a given schema are deployed as DT instances. The DTC Manager (right) is composed by a middleware and an orchestrator. The first is responsible for managing the registry of DTs and PAs, while the second is responsible for orchestrating the deployment of DTs across the DTC nodes.}
    \label{fig:dtc-architecture}
\end{figure}

The aim of the DTC is to create a new open-system perspective of connected, interoperable and pervasive DTs modeled and engineered to create an effective cyber-physical abstraction layer.
%
This requires, first, facilitating the execution of DTs on a set of DT platforms.
Second, by exploiting the advantages of a broad compute continuum of cloud and edge resources, to deploy the DTs on the \textit{best} Compute Node (CN) available to guarantee application requirements.

%----------------------------------------
\subsection{Functional Overview}
%----------------------------------------

The architecture of the DTC is depicted in \Cref{fig:dtc-architecture} and includes DTC functional blocks (grayed boxes), infrastructure modules, DT artifacts, and DTC stakeholders. From a high-level perspective, the DTC allows DT operators to expose a collection of \textit{M} DTs for use by DT consumers. Each DT is described by a schema, which in turn is associated with a number of DT packages (up to \textit{N}), enabling deployment on different platforms. New DT schemas and packages can be dynamically onboarded by DT developers.

In the DTC, we distinguish between \emph{compute nodes (CNs)} and \emph{DTC nodes}. 
A compute node represents an individual unit of computation (e.g., a VM, a bare-metal server, or a container host) where a DT instance is executed. 
A DTC node instead abstracts a collection of compute nodes orchestrated by the same Virtual Infrastructure Manager (VIM), similarly to how a MEC node in ETSI MEC represents a set of compute hosts under a common VIM. Typical examples include Kubernetes clusters or OpenStack-managed virtualized clusters.

Within each DTC node, the DTC Agent is responsible for managing and monitoring the execution of DTs on the underlying compute nodes. 
The VIM handles low-level resource management, while the DTC Agent exposes the control and monitoring capabilities needed by the DTC Manager to orchestrate DT execution and placement across the continuum. 
Above these components, the DTC Manager coordinates all DTC Agents to provide a unified operational interface over the entire infrastructure.

DT packages are used to create and execute DT instances on top of DT platforms. 
Each instance runs on a specific compute node inside a DTC node. 
\acp{DT} are deployed across the compute continuum by first selecting the appropriate DTC node (i.e., the cluster orchestrated by a VIM) and then, within it, the compute node that will host the instance. 
This separation enables the DTC Orchestrator to move and replicate DTs not only across compute nodes but also across different DTC nodes, depending on fidelity requirements, resource availability, and performance considerations.

More in detail, a DT operator requesting the creation of a DT corresponding to a given schema contacts the DTC Manager, which is responsible for identifying suitable platforms and corresponding DT packages, based on fidelity and connectivity requirements, and for configuring communication and computing resources accordingly. 
The DTC Manager comprises two logical components: the DTC Middleware and the DTC Orchestrator. 
The Middleware acts as the entry point for all stakeholder requests and configures DTs directly when possible. 
The Orchestrator is responsible for instantiating DTs by interacting with DTC Agents to configure the necessary resources through the underlying VIMs. 
This separation enables the DTC to support heterogeneous underlying infrastructures while providing a unified management layer.

%----------------------------------------
\subsection{Interaction Example}
%----------------------------------------


\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{figures/dtc/sequence_diagram_v2.pdf}
    \caption{The interaction flow example between DTC architectural components to enable DT registration and deployment.}
    \label{fig:dtc-interaction}
\end{figure}

\Cref{fig:dtc-interaction} presents an illustrative example depicting a typical interaction scenario among the key components of the DTC architecture.
%
Assuming that the PA has been already registered in the DTC providing the corresponding PAS and PAI descriptions, the figure illustrates the sequence of interactions that occur when a generic user aims to register and deploy a DT for a specific PA.

The initial step to deploy a DT on a target platform involves registering a new DT Schema.
%
All the interactions are facilitated by the DTC Manager, which implements the external-facing API and receives, handles and forwards incoming requests to the appropriate internal component.
%
The request to register a DT Schema is forwarded from the DTC Manager to the DT Middleware. Here, the incoming request is validated, and a new schema is created for the target DT associated with its description, capabilities and fidelity together with the type of physical asset. Subsequently, the DTC manager responds to the user with positive feedback.

The subsequent step entails registering or creating a DT Package linked to the specific schema. The interaction flow follows is similar to the one above, with the DTC Manager receiving the request and the DT Middleware registering the package and associating it with the corresponding SchemaID. 

Moving forward, the registration process for the DTC User involves identifying suitable compute nodes for deploying the target DT. This necessitates a node discovery request initiated by the user, which is forwarded to the DTC Manager.
%
Internally, the DT Orchestrator engages in the discovery process, returning a list of available nodes that match the user's specified criteria, such as the support for package deployment and the support for target hardware requirements (e.g., a GPU for the execution of specific DT's functionalities).
%
The selection of nodes could be automated based on predefined policies. For simplicity, in this example, the user chooses the preferred node from the list of available options.

Upon reviewing the list of available compute nodes matching the target package and schema, the user can proceed by sending a start request to the DTC Manager.
%
This request includes the specific package ID associated with the target schema, along with the starting configuration of the twin which include the identifier of the PAI to which the \ac{DT} will be linked together with the node ID indicating where the DT should be deployed. 
The DTC Manager first verifies compliance of the \ac{DT} package with the platform and the selected PAI, then it checks for the availability of the designated DTC Agent managing the specified node.

Subsequently, it forwards a start request to the target DTC Agent managing the selected platform and node.
%
Here, the DTC Agent checks for the presence of the target DT Package and downloads it if necessary. Once the package is available, the Agent initiates and starts the target DT on the designated platform.
%
Finally, the Agent receives the instance ID of the running DT from the platform and communicates it back to the DTC manager. Subsequently, the DTC manager replies with the instance ID to the user, along with the current description of the running DT instance.

This interaction flow exemplifies the collaborative efforts of the DTC components in facilitating the registration and deployment of DTs, ensuring a seamless experience for users while abstracting the underlying complexities of the infrastructure.
%
By leveraging the descriptions introduced in \Cref{table:dtc_descriptions_requirements}, the DTC effectively manages the lifecycle of DTs, from registration to deployment, while ensuring compatibility with the underlying platforms and compute nodes.

%=======================================================
\section{Proof of Concept Implementation}
%=======================================================

A preliminary analysis, implementation, and experimental evaluation of the core functionalities of the DTC have been conducted using a prototype based on Eclipse Ditto and WLDT as reference platforms.
%
The primary objective of this implementation is to demonstrate the feasibility of the DTC and obtain initial insights by leveraging the microservice capabilities of the White Label Digital Twin (WLDT) library.
%
The DTC's functional modules (\Cref{fig:dtc-architecture}) are implemented in Python, featuring a \emph{master API} that orchestrates overall behavior and a set of \emph{DTC agents} that manage specific computing nodes and interface with underlying DT platforms: namely, WLDT and Eclipse Ditto for this experimental setting. 
%
These components expose structured RESTful APIs, enabling standardized interaction with the DTC core services. Monitoring and performance tracking across distributed nodes are facilitated through Prometheus, serving as a centralized time-series database, while local agents collect metrics on individual machines.
%
Observability, visualization, and analysis are supported via Grafana dashboards. For the experimental evaluation, the supporting infrastructure -- including Virtual Machines (VMs), networking, and container runtimes (Docker and Kubernetes) -- was assumed to be preconfigured.

The Edge and Cloud infrastructures are associated with two real Proxmox clusters and deployments.
The Edge node is realized as a server within the same laboratory as the microfactory used as the source of \ac{PA} data, hosting the industrial Edge nodes.
%
The microfactory setup is the same used in \Cref{ssec:dte:dt-engineering:scenario}.
Conversely, the Cloud component is realized using a second Proxmox cluster external to the laboratory network and connected via the Internet, ensuring that delays and latencies are aligned with non-local communication scenarios.
On the respective Proxmox clusters, Linux Ubuntu Server 20.04 VMs were deployed, upon which the containerization environments were installed for the execution of all required applications, including Ditto, WLDT, and the monitoring and logging layers. 
Specifically, the Edge infrastructure utilized a VM provisioned with 2 CPUs, 2 GB of RAM, and 20 GB of storage, while the Cloud infrastructure was realized on a VM featuring 8 CPUs, 8 GB of RAM, and 500 GB of storage.

The \textbf{first experiment} examines the behavior of two \acp{DT} deployed across two platforms: WLDT at the edge and Ditto in the cloud. Rather than focusing on communication delays, this experiment aims to demonstrate how the DTC orchestrates state computation and ensures synchronization across multiple DT instances operating on heterogeneous platforms. Two observers, one on the edge and one in the cloud, monitored the propagation of \ac{PA} and DT states.
%
In the first configuration (\Cref{fig:exp_single_dt:same_model}), both DTs run the same model version. The results confirm that the DTC effectively deploys the two instances which are getting data from the same physical asset and compute the state, with the expected delay difference between edge and cloud platforms.
%
To each \ac{PA} state update (green), the edge DT (WLDT) computes the new state (blue). Similarly the cloud DT (Ditto) receives the updates and computes its state (orange). The DTC ensures that both DTs are correctly deployed and synchronized, demonstrating its capability to manage multiple DT instances across heterogeneous platforms.

In the second configuration (\Cref{fig:exp_single_dt:coarse_model}), the cloud-based DT is switched to a different, coarser, DT schema (V2) that employs a longer time window for state aggregation (receiving updates from the PA every seconds and computing the state every 5 samples), while the edge DT continues operating with the original high-granularity schema.
%
The DTC seamlessly manages this update, ensuring the cloud DT is correctly deployed and integrated without disrupting the ongoing edge operations. This illustrates the DTC's capability to orchestrate heterogeneous DT versions.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}\label{fig:exp_single_dt:same_model}
        \includegraphics[width=\textwidth]{figures/dtc/wldt_ditto_end_to_end_delay_same_schema_state_comp.pdf}
        \caption{}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.49\textwidth}\label{fig:exp_single_dt:coarse_model}
        \includegraphics[width=\textwidth]{figures/dtc/wldt_ditto_end_to_end_delay_diff_schema_state_comp.pdf}
        \caption{}
    \end{subfigure}
    \caption{Comparison of end-to-end DT state computation delays across edge (WLDT) and cloud (Ditto) platforms.(a) shows the same DT model being deployed on both edge and cloud, while (b) shows a coarser DT model being deployed on the cloud. Below each figure, the number of DT states computed in the last 10 seconds, to highlight the difference in the update frequency of the two DTs.}
    \label{fig:exp_single_dt}
\end{figure*}

The first experiment was engineered to address a realistic operational and architectural challenge facing stakeholders in industrial DT deployments:
the management of asymmetric data granularity and synchronization requirements across heterogeneous Edge and Cloud environments.
This asymmetry reflects a core business trade-off: high data frequency is strategic at the Edge to support time-critical applications, such as real-time predictive maintenance and rapid feedback loops, demanding maximum responsiveness.
Conversely, a lower synchronization rate is often sufficient for the Cloud layer, typically supporting historical analysis, aggregate reporting, or non-critical dashboards.
This selective reduction in frequency -- a direct response to the stakeholder need to save bandwidth, storage costs, and processing load in the Cloud -- is a key driver.
The DTC is validated in this context by demonstrating its capability to deploy different DT schemas across distinct platforms.

The \textbf{second experiment} evaluates the scalability of the DTC prototype in a realistic DT ecosystem deployment involving multiple DTs that work together to digitalize the microfactory.
%
This scenario directly addresses a key stakeholder challenge:
the need for a phased and controlled deployment where visibility into the factory floor is progressively increased.

The deployment evolved incrementally over the experimental timeline,
as shown in Figure \ref{fig:dt_resource_usage_dt_count}, 
which depicts the number of active DTs on the edge and cloud platforms.
%
Specifically, eight DTs were first launched on the edge, followed by the activation of four additional DTs in the cloud, achieving full deployment of all twelve DT types present in the Microfactory.
%
Later, edge DTs are also replicated in the cloud and vice-versa, to simulate a scenario, where replicas are needed to support different use cases and applications. Bringing the total to 24 DTs running across the two platforms.

In this configuration, the edge DTs exhibited extended functional behavior compared to the first experiment.

They implemented more advanced DT capabilities, specifically augmenting the PA digitalization by performing complex tasks such as industrial machine state computation and anomaly detection for blocked products within the production line.
Critically, the cloud DTs received their state input indirectly from their corresponding edge DTs, rather than retrieving data directly from the physical assets in order to give to the edge DTs the responsibilities of the first digitalization points and controlling data flows and granularity.
This setup highlights the DTC's ability to coordinate multiple DT instances across heterogeneous platforms, maintaining synchronization, ensuring consistent communication, and optimizing resource utilization as the system scales.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/dtc/edge_cloud_experiment_dt_count.pdf}
    \caption{Number of active DT instances on the Edge and Cloud platforms during the incremental deployment process. The graph illustrates the gradual increase in the number of DTs over time, reflecting the phased deployment strategy adopted in the experiment.}
    \label{fig:dt_resource_usage_dt_count}
\end{figure}

Performance metrics—including CPU usage, memory consumption, and network traffic—were continuously monitored during this scaling phase. The primary objective is to demonstrate the flexibility and robustness of the proposed DTC system in seamlessly coordinating and orchestrating these evolutionary and incremental deployments in a realistic, heterogeneous environment.
%
It is important to emphasize that Ditto manages multiple DTs within a single centralized platform composed of several core microservices (e.g., connectivity, storage, proxy, query, gateway), whereas WLDT deploys each DT as an independent microservice at the Edge.
Consequently, in Ditto, the deployment of multiple DTs affects the overall platform performance, while in WLDT, each DT instance can be monitored independently.
This fundamental difference in platform architecture introduces additional complexity, which is transparently handled by the DTC.
By abstracting these platform-specific details, the DTC allows application designers to focus on defining schemas, packages, and the desired behaviors to be implemented, without needing to manage the underlying deployment differences.
%
During the experiment, detailed measurements were collected for CPU and memory usage, along with inbound and outbound network traffic, for each active microservice---including the core components of Eclipse Ditto and the WLDT-based DT instances. 
These metrics provide insights into how system resources are utilized as the number of active DTs increases across Edge and Cloud deployments. 
The results, illustrated in Figure \ref{fig:dt_resource_usage}, reveal the distinct operational phases of the experiment and demonstrate the DTC's ability to coordinate and manage multiple heterogeneous platforms concurrently.

%%%
\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{figures/dtc/edge_cloud_experiment.pdf}
    \caption{Evaluation of the DTC prototype enabling multi-platform DT deployment in the Microfactory. The figure compares resource usage (CPU, Memory, Network Sent and Received) for Ditto in the Cloud (top row) hosting multiple DTs within a single Ditto instance composed of several microservices (Conntectivity, UI, Gateway, Nginx, Policies, Things, Things-Search) and WLDT at the Edge (bottom row) deploying individual DTs as microservices (DT1,..., DT12) as the number of active DTs increases.}
    \label{fig:dt_resource_usage}
\end{figure}
%%%

The obtained results underscore the feasibility of dynamically, modularly, and flexibly evolving an Edge-Cloud Digital Twin deployment within a realistic industrial context, thanks to the DTC's structured and integrated approach.
This successful evolution demonstrates the system's capacity to accommodate new DTs over time, 
effectively matching emerging stakeholder requirements and catering to different observers and users of the DTs across local and remote Cloud environments.
%
Crucially, the proposed DTC solution effectively masks the complexities of the various underlying platforms (WLDT and Ditto). 
Without such an integrated orchestration layer, the deployment, synchronization, and lifecycle management of these heterogeneous, cross-platform DT instances would entail a significantly greater level of complexity and manual effort for developers and system operators. 
This abstraction layer allows users to seamlessly select the DTs of interest based on the architectural layer where they need to be deployed, the specific features and functionalities required, and the data sources to be utilized.

Overall, the experiment confirms both the feasibility and robustness of the DTC architecture.
The results show that the DTC can seamlessly encapsulate the heterogeneity of underlying Digital Twin platforms,
offering a unified and coherent system view across deployment layers.
Furthermore, it maintains scalability, observability, and consistent system performance as operational load and deployment complexity increase—demonstrating its suitability for large-scale, distributed industrial scenarios


%=======================================================
\section{Final Remarks}
%=======================================================

This chapter presents the concept of \acl{DTC} as a middleware platform to support the operational management of \ac{DTE} on a compute continuum. 
%
The \ac{DTC} addresses key challenges that emerge when dealing with the deployment and management of systems that involve multiple \acp{DT}, characterized by heterogeneous physical assets, diverse DT platforms, and varying application requirements.

The proposal in this chapter contributes to answering the research question:

\paragraph{\ref{rq:3} How to support the operational management of DTEs?}

By using structured descriptions that capture the essential characteristics of physical assets, digital twins, and runtime platforms, the DTC enables stakeholders to effectively discover, deploy, and manage DT instances across a distributed infrastructure.
%
The descriptions are designed to be platform and application-agnostic, allowing for seamless integration and interoperability among different DT platforms and use cases.
%
The DTC architecture incorporates key functional components, including a middleware layer and orchestrator, which facilitate the interaction with distributed computing nodes through the abstraction of DTC agents. 
%
This approach supports different stakeholders in their respective roles, offering a coherent holistic view of the DT ecosystem alongside the development-to-deployment lifecycle of its components. 

